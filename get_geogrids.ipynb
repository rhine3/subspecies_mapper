{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import h3\n",
    "import folium\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "pd.options.mode.copy_on_write = True \n",
    "from functools import reduce\n",
    "from folium import GeoJsonTooltip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON of subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAXON_ORDER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>TAXON_CONCEPT_ID</th>\n",
       "      <th>PRIMARY_COM_NAME</th>\n",
       "      <th>SCI_NAME</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>SPECIES_GROUP</th>\n",
       "      <th>REPORT_AS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common Ostrich</td>\n",
       "      <td>Struthio camelus</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Somali Ostrich</td>\n",
       "      <td>Struthio molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>slash</td>\n",
       "      <td>y00934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common/Somali Ostrich</td>\n",
       "      <td>Struthio camelus/molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>species</td>\n",
       "      <td>soucas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southern Cassowary</td>\n",
       "      <td>Casuarius casuarius</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>species</td>\n",
       "      <td>dwacas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dwarf Cassowary</td>\n",
       "      <td>Casuarius bennetti</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TAXON_ORDER CATEGORY SPECIES_CODE  TAXON_CONCEPT_ID       PRIMARY_COM_NAME  \\\n",
       "0            2  species      ostric2               NaN         Common Ostrich   \n",
       "1            7  species      ostric3               NaN         Somali Ostrich   \n",
       "2            8    slash       y00934               NaN  Common/Somali Ostrich   \n",
       "3           10  species      soucas1               NaN     Southern Cassowary   \n",
       "4           11  species      dwacas1               NaN        Dwarf Cassowary   \n",
       "\n",
       "                         SCI_NAME             ORDER  \\\n",
       "0                Struthio camelus  Struthioniformes   \n",
       "1          Struthio molybdophanes  Struthioniformes   \n",
       "2  Struthio camelus/molybdophanes  Struthioniformes   \n",
       "3             Casuarius casuarius    Casuariiformes   \n",
       "4              Casuarius bennetti    Casuariiformes   \n",
       "\n",
       "                              FAMILY        SPECIES_GROUP REPORT_AS  \n",
       "0          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "1          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "2          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "3  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "4  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load eBird taxonomy\n",
    "taxonomy = pd.read_csv(\"eBird_taxonomy_v2024.csv\")\n",
    "taxonomy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['species', 'slash', 'issf', 'hybrid', 'spuh', 'domestic', 'form',\n",
       "       'intergrade'], dtype=object)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What categories are there in the taxonomy?\n",
    "taxonomy.CATEGORY.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11145"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many species are there?\n",
    "species = taxonomy[taxonomy.CATEGORY == 'species']\n",
    "len(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3843"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many infraspecific entries are there?\n",
    "infrasp_categories = ['issf', 'form', 'intergrade']\n",
    "infraspp = taxonomy[taxonomy.CATEGORY.isin(infrasp_categories)]\n",
    "len(infraspp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping species to their infraspecies, by category\n",
    "# Most species have a single category of infraspecies (e.g. either form or subspecies, not both)\n",
    "# However some species have infraspecies in multiple categories, e.g. Brant (Branta bernicla) has both subspecies and forms\n",
    "spp_dict = species[['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "# Add infraspecies to spp_json\n",
    "for sp in tqdm(spp_dict.keys()):\n",
    "    # Get infraspecies for this species\n",
    "    infraspp_for_sp = infraspp[infraspp['SCI_NAME'].apply(lambda x: x[:len(sp)] == sp)]\n",
    "    infraspp_dict = dict()\n",
    "\n",
    "    # Add infraspecies to spp_json by category\n",
    "    for cat in infrasp_categories:\n",
    "        infrasp_in_category = infraspp_for_sp[infraspp_for_sp.CATEGORY == cat]\n",
    "        infrasp_cat_dict = infrasp_in_category[\n",
    "            ['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "        if len(infrasp_cat_dict.keys()) > 0:\n",
    "            infraspp_dict[cat] = infrasp_cat_dict\n",
    "    spp_dict[sp]['infraspecies'] = infraspp_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"infraspecies_ebird.json\", 'w') as f:\n",
    "    f.write(json.dumps(spp_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"infraspecies_ebird.json\") as f:\n",
    "    spp_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep eBird data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following H3 resolutions:\n",
    "```\n",
    "Res\tAverage Hexagon Area (km2)\tPentagon Area* (km2)\tRatio (P/H)\n",
    "2\t86,801.780398997\t44,930.898497879\t0.5176\n",
    "3\t12,393.434655088\t6,315.472267516\t0.5096\n",
    "4\t1,770.347654491\t896.582383141\t0.5064\n",
    "5\t252.903858182\t127.785583023\t0.5053\n",
    "```\n",
    "\n",
    "They have this many cells:\n",
    "```\n",
    "Res\tTotal number of cells\tNumber of hexagons\tNumber of pentagons\n",
    "2\t5,882\t5,870\t12\n",
    "3\t41,162\t41,150\t12\n",
    "4\t288,122\t288,110\t12\n",
    "5\t2,016,842\t2,016,830\t12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine number of sightings of each subspecies per grid cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing strher\n",
      "No more data to process, total rows in dataset:  600000\n",
      "\n",
      "\n",
      "\n",
      "Processing easmea\n",
      "No more data to process, total rows in dataset:  2500000\n",
      "\n",
      "\n",
      "\n",
      "Processing yerwar\n",
      "No more data to process, total rows in dataset:  12300000\n",
      "\n",
      "\n",
      "\n",
      "Processing eurjay1\n",
      "No more data to process, total rows in dataset:  1300000\n",
      "\n",
      "\n",
      "\n",
      "Processing brant\n",
      "No more data to process, total rows in dataset:  900000\n",
      "\n",
      "\n",
      "\n",
      "Processing whcspa\n",
      "No more data to process, total rows in dataset:  6500000\n",
      "\n",
      "\n",
      "\n",
      "Processing cacgoo1\n",
      "No more data to process, total rows in dataset:  900000\n",
      "\n",
      "\n",
      "\n",
      "Processing horlar\n",
      "No more data to process, total rows in dataset:  2000000\n",
      "\n",
      "\n",
      "\n",
      "Processing coatit2\n",
      "No more data to process, total rows in dataset:  900000\n",
      "\n",
      "\n",
      "\n",
      "Processing foxspa\n",
      "No more data to process, total rows in dataset:  2000000\n",
      "\n",
      "\n",
      "\n",
      "Processing daejun\n",
      "No more data to process, total rows in dataset:  14300000\n",
      "\n",
      "\n",
      "\n",
      "Processing orcwar\n",
      "Last chunk, total rows in dataset: 2900001\n",
      "\n",
      "\n",
      "\n",
      "Processing yebcha\n",
      "Last chunk, total rows in dataset: 1000001\n",
      "\n",
      "\n",
      "\n",
      "Processing cangoo\n",
      "Last chunk, total rows in dataset: 20300001\n",
      "\n",
      "\n",
      "\n",
      "Processing whiwag\n",
      "Last chunk, total rows in dataset: 2800001\n",
      "\n",
      "\n",
      "\n",
      "Processing norfli\n",
      "Last chunk, total rows in dataset: 14700001\n",
      "\n",
      "\n",
      "\n",
      "Processing rethaw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/3724824567.py:132: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n",
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/3724824567.py:132: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 11778727\n",
      "\n",
      "\n",
      "\n",
      "Processing comeid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/3724824567.py:132: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 1072050\n",
      "\n",
      "\n",
      "\n",
      "Processing perfal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/3724824567.py:132: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n",
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/3724824567.py:132: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 1759264\n",
      "\n",
      "\n",
      "\n",
      "Processing redcro\n",
      "Last chunk, total rows in dataset: 797057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resolutions = [2,3,4,5]\n",
    "def clean_ebd(\n",
    "    full_df,\n",
    "    remove_unconfirmed=True, \n",
    "    remove_reviewed=False,\n",
    "    resolutions = resolutions,\n",
    "):\n",
    "\n",
    "    # Remove duplicate checklists\n",
    "    full_df = full_df[full_df['GROUP IDENTIFIER'].isnull() | ~full_df[full_df['GROUP IDENTIFIER'].notnull()].duplicated(subset=[\"GROUP IDENTIFIER\", \"SCIENTIFIC NAME\"],keep='first')]\n",
    "\n",
    "    # Removed unconfirmed observations or reviewed observations, if desired\n",
    "    if remove_unconfirmed:\n",
    "        full_df = full_df[full_df[\"APPROVED\"] == 1]\n",
    "    if remove_reviewed:\n",
    "        full_df = full_df[full_df[\"REVIEWED\"] == 0]\n",
    "\n",
    "    # Just subset to the needed columns\n",
    "    needed_columns = [\n",
    "        'TAXONOMIC ORDER','CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', \n",
    "        'SCIENTIFIC NAME','SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "        'SAMPLING EVENT IDENTIFIER',\n",
    "        'LATITUDE', 'LONGITUDE', 'REVIEWED', 'OBSERVATION DATE']\n",
    "    full_df = full_df[needed_columns]\n",
    "    full_df.head()\n",
    "\n",
    "    # Convert latitude and longitude to an H3 hexagon ID\n",
    "    for resolution in resolutions:\n",
    "        full_df[f'hex_id_{resolution}'] = full_df.apply(lambda row:  h3.latlng_to_cell(row.LATITUDE, row.LONGITUDE, resolution), axis=1)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "def get_grid_cell_species_data(cell_df, sp, subspp, resolution):\n",
    "    \"\"\"Get # of checklists containing a species and each subspecies\n",
    "\n",
    "    Args:\n",
    "    - cell_df: pd.DataFrame, dataframe of data for a single grid cell (1 row per observation)\n",
    "    - sp: str, scientific name of species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "\n",
    "    Returns:\n",
    "    - cell_data: dict, with keys 'cell_id', species name, and subspecies names\n",
    "    \"\"\"\n",
    "    # Total number of checklists containing the species\n",
    "    num_checklists = cell_df[\"SAMPLING EVENT IDENTIFIER\"].nunique()\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_data = {'cell_id': cell_df[f\"hex_id_{resolution}\"].iloc[0]}\n",
    "    cell_data[sp] = num_checklists\n",
    "\n",
    "    # Add number of checklists containing each subspecies\n",
    "    for subsp in subspp:\n",
    "        num_subsp = cell_df[cell_df[\"SUBSPECIES SCIENTIFIC NAME\"] == subsp].shape[0]\n",
    "        cell_data[subsp] = num_subsp\n",
    "\n",
    "    return cell_data\n",
    "\n",
    "\n",
    "def get_species_df(sp, sp_df, subspp, resolution):\n",
    "    \"\"\"Make dataframe of species & subspecies data for every cell for a given species\n",
    "\n",
    "    Args:\n",
    "    - sp: str, scientific name of species\n",
    "    - df: pd.DataFrame, dataframe of data for this species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "    - resolution: int, H3 resolution level\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_dicts = []\n",
    "    for cell in sp_df[f\"hex_id_{resolution}\"].unique():\n",
    "        cell_df = sp_df[sp_df[f\"hex_id_{resolution}\"] == cell]\n",
    "        cell_data = get_grid_cell_species_data(cell_df, sp, subspp, resolution)\n",
    "        cell_dicts.append(cell_data)\n",
    "\n",
    "    sp_cell_df = pd.DataFrame(cell_dicts, index=range(len(cell_dicts)))\n",
    "    sp_cell_df.set_index(\"cell_id\", inplace=True)\n",
    "\n",
    "    return sp_cell_df\n",
    "\n",
    "\n",
    "\n",
    "spp_dict = json.load(open(\"infraspecies_ebird.json\"))\n",
    "\n",
    "# caja_df = get_species_df(sp, full_df, subspp_dict)\n",
    "# filepath = Path('batches').joinpath(filename)\n",
    "# caja_df.to_csv(filepath)\n",
    "\n",
    "#dataset_filepath = \"ebd-sample.txt\"\n",
    "#sp_code = 'rethaw'\n",
    "use_cols = [\n",
    "        'TAXONOMIC ORDER','CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', \n",
    "        'SCIENTIFIC NAME','SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "        'SAMPLING EVENT IDENTIFIER',\n",
    "        'LATITUDE', 'LONGITUDE', 'REVIEWED', 'APPROVED', 'GROUP IDENTIFIER', 'OBSERVATION DATE']\n",
    "\n",
    "sp_codes = [x.name.split('_')[1] for x in list(Path(\"data/\").glob(\"*.zip\"))]\n",
    "for sp_code in sp_codes:\n",
    "    print(\"\\n\\n\\nProcessing\", sp_code)\n",
    "    dataset_filepath = f\"data/ebd_{sp_code}_relOct-2024/ebd_{sp_code}_relOct-2024.txt\"\n",
    "\n",
    "    resolution = resolutions[0]\n",
    "\n",
    "    ssp_batch_directory = Path('batches/')\n",
    "    ssp_batch_directory.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read in CSV in batches\n",
    "    chunk_rows = 100000\n",
    "    tracker_filepath = f\"{sp_code}_tracker_rowsperchunk-{chunk_rows}.csv\"\n",
    "\n",
    "    if Path(tracker_filepath).exists():\n",
    "        tracker = pd.read_csv(tracker_filepath)\n",
    "        tracker[\"spp_to_do\"] = tracker[\"spp_to_do\"].apply(ast.literal_eval) \n",
    "        tracker[\"spp_done\"] = tracker[\"spp_done\"].apply(ast.literal_eval) \n",
    "        start_idx = tracker.index[-1]\n",
    "        spp_to_do = set(tracker.loc[start_idx].spp_to_do) - set(tracker.loc[start_idx].spp_done)\n",
    "        if spp_to_do == set():\n",
    "            skiprows = tracker.loc[start_idx].end_row\n",
    "            start_idx = start_idx + 1\n",
    "            spp_to_do = None\n",
    "        else:\n",
    "            skiprows = tracker.loc[start_idx].start_row\n",
    "\n",
    "    else:\n",
    "        tracker = pd.DataFrame(columns=[\"start_row\", \"end_row\", \"spp_to_do\", \"spp_done\"])\n",
    "        start_idx = 0\n",
    "        spp_to_do = None\n",
    "        skiprows=0\n",
    "\n",
    "    # TODO: DEAL WITH BUG (BELOW)\n",
    "    # SWITCH TO DASK TO PARALLELIZE\n",
    "    for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n",
    "        if chunk.shape[0] == 0:\n",
    "            print(\"No more data to process, total rows in dataset: \", (start_idx + idx)*chunk_rows)\n",
    "            break\n",
    "        if chunk.shape[0] < chunk_rows:\n",
    "            # Some kind of weird bug/issue with the last chunk \n",
    "            # which finds a single row left to process claiming to be in the next 100,000 rows after the last one\n",
    "            # This only happens after the first time I rerun this cell\n",
    "            end_row = (start_idx+idx)*chunk_rows + chunk.shape[0]\n",
    "            print(f\"Last chunk, total rows in dataset:\", end_row)\n",
    "        else:\n",
    "            end_row = (start_idx + idx)*chunk_rows+chunk_rows\n",
    "        cleaned = clean_ebd(chunk)\n",
    "        if spp_to_do == None: # Add new row\n",
    "            spp_to_do = list(set(cleaned[\"SCIENTIFIC NAME\"].unique()))\n",
    "            tracker.loc[start_idx+idx] = [(start_idx + idx)*chunk_rows, end_row, spp_to_do, []]\n",
    "\n",
    "        \n",
    "        for sp in spp_to_do:\n",
    "            cleaned_sp = cleaned[cleaned[\"SCIENTIFIC NAME\"] == sp]\n",
    "            if cleaned_sp.shape[0] == 0:\n",
    "                #print(f\"No data for {sp}\")\n",
    "                continue\n",
    "\n",
    "            # Get list of subspecies\n",
    "            subspp = []\n",
    "            for k, val in spp_dict[sp]['infraspecies'].items():\n",
    "                subspp.extend(val.keys())\n",
    "            \n",
    "            # Get data on presence of each subspp for each resolution\n",
    "            for resolution in resolutions:\n",
    "                species_df = get_species_df(sp, cleaned_sp, subspp, resolution)\n",
    "                filename = ssp_batch_directory.joinpath(f'{sp}_row{(start_idx+idx)*chunk_rows}-{end_row}_resolution{resolution}.csv')\n",
    "                species_df.to_csv(filename)\n",
    "\n",
    "            tracker.loc[start_idx+idx].spp_done += [sp]\n",
    "            tracker.to_csv(tracker_filepath, index=False)\n",
    "\n",
    "        spp_to_do = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum up the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cell_df_directory = Path('sp_cell_dfs/')\n",
    "sp_cell_df_directory.mkdir(exist_ok=True)\n",
    "\n",
    "def parse_batch_files(ssp_batch_directory):\n",
    "    batch_files = list(ssp_batch_directory.glob(\"*.csv\"))\n",
    "    file_info = [n.name.split(\"_\") for n in batch_files]\n",
    "    files = pd.DataFrame(file_info, columns=['SCIENTIFIC NAME', 'ROW RANGE', 'RESOLUTION'])\n",
    "    files['FILENAME'] = batch_files\n",
    "    for (species, resolution), species_df in files.groupby([\"SCIENTIFIC NAME\", 'RESOLUTION']):\n",
    "        species = species.replace(\" \", \"-\")\n",
    "        resolution = resolution[:-4]\n",
    "        all_dataframes = [pd.read_csv(f, index_col=0) for f in species_df.FILENAME] \n",
    "        sp_cell_df = reduce(lambda a, b: a.add(b, fill_value=0), all_dataframes)\n",
    "        filename = sp_cell_df_directory.joinpath(f'{species}_{resolution}.csv')\n",
    "        sp_cell_df.to_csv(filename)\n",
    "\n",
    "\n",
    "\n",
    "parse_batch_files(ssp_batch_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_color_too_similar(150, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping strher\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_68552/1194888260.py:277: RuntimeWarning: divide by zero encountered in divide\n",
      "  return total_overlap / max_overlap\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 542\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    540\u001b[0m     subspp_colors \u001b[38;5;241m=\u001b[39m get_color_mapping(sp_cell_df)\n\u001b[0;32m--> 542\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mchoropleth_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp_cell_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubspp_colors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs/maps/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecies\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[90], line 450\u001b[0m, in \u001b[0;36mchoropleth_map\u001b[0;34m(sp_cell_df, common_name, subspp_colors)\u001b[0m\n\u001b[1;32m    447\u001b[0m percentages_dict \u001b[38;5;241m=\u001b[39m percentages\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Precompute tooltip text showing only non-zero percentages\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m percentages_dict_ordered \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpercentages_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpct > 0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    452\u001b[0m tooltip_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subsp, percent \u001b[38;5;129;01min\u001b[39;00m percentages_dict_ordered\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/frame.py:4823\u001b[0m, in \u001b[0;36mDataFrame.query\u001b[0;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[1;32m   4821\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4822\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4823\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4825\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   4826\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[res]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/frame.py:4949\u001b[0m, in \u001b[0;36mDataFrame.eval\u001b[0;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[1;32m   4946\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   4947\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolvers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolvers\u001b[39m\u001b[38;5;124m\"\u001b[39m, ())) \u001b[38;5;241m+\u001b[39m resolvers\n\u001b[0;32m-> 4949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/computation/eval.py:357\u001b[0m, in \u001b[0;36meval\u001b[0;34m(expr, parser, engine, local_dict, global_dict, resolvers, level, target, inplace)\u001b[0m\n\u001b[1;32m    355\u001b[0m eng \u001b[38;5;241m=\u001b[39m ENGINES[engine]\n\u001b[1;32m    356\u001b[0m eng_inst \u001b[38;5;241m=\u001b[39m eng(parsed_expr)\n\u001b[0;32m--> 357\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43meng_inst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed_expr\u001b[38;5;241m.\u001b[39massigner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_line:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/computation/engines.py:81\u001b[0m, in \u001b[0;36mAbstractEngine.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maligned_axes \u001b[38;5;241m=\u001b[39m align_terms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr\u001b[38;5;241m.\u001b[39mterms)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# make sure no names in resolvers and locals/globals clash\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reconstruct_object(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_type, res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maligned_axes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr\u001b[38;5;241m.\u001b[39mterms\u001b[38;5;241m.\u001b[39mreturn_type\n\u001b[1;32m     84\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/computation/engines.py:121\u001b[0m, in \u001b[0;36mNumExprEngine._evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m scope \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mfull_scope\n\u001b[1;32m    120\u001b[0m _check_ne_builtin_clash(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/numexpr/necompiler.py:973\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m e \u001b[38;5;241m=\u001b[39m validate(ex, local_dict\u001b[38;5;241m=\u001b[39mlocal_dict, global_dict\u001b[38;5;241m=\u001b[39mglobal_dict, \n\u001b[1;32m    970\u001b[0m              out\u001b[38;5;241m=\u001b[39mout, order\u001b[38;5;241m=\u001b[39morder, casting\u001b[38;5;241m=\u001b[39mcasting, \n\u001b[1;32m    971\u001b[0m              _frame_depth\u001b[38;5;241m=\u001b[39m_frame_depth, sanitize\u001b[38;5;241m=\u001b[39msanitize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_frame_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frame_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/numexpr/necompiler.py:1004\u001b[0m, in \u001b[0;36mre_evaluate\u001b[0;34m(local_dict, _frame_depth)\u001b[0m\n\u001b[1;32m   1002\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _numexpr_last[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m evaluate_lock:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon, mapping\n",
    "from shapely.ops import split\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import networkx as nx\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import colorsys\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_infraspecies_relationships(sp, spp_dict=spp_dict):\n",
    "    data = spp_dict[sp]['infraspecies']\n",
    "    \n",
    "    # Get a list of each type of infraspecies\n",
    "    if \"issf\" in data.keys():\n",
    "        issfs = [k.replace(sp+' ', '') for k in data[\"issf\"].keys()] # Recognized ssp or ssp groups\n",
    "    else:\n",
    "        issfs = []\n",
    "    if \"form\" in data.keys():\n",
    "        forms = [k.replace(sp+' ', '') for k in data[\"form\"].keys()] # Forms\n",
    "    else:\n",
    "        forms = []\n",
    "    if \"intergrade\" in data.keys():\n",
    "        intergrades = [k.replace(sp+' ', '') for k in data[\"intergrade\"].keys()] # Intergrades (between ssp? forms?)\n",
    "    else:\n",
    "        intergrades = []\n",
    "\n",
    "    intergrade_to_parents = dict()\n",
    "    forms_to_parents = dict()\n",
    "    top_level_intergrades = []\n",
    "    top_level_forms = []\n",
    "\n",
    "    # Find parents of the intergrades, if any are in the eBird taxonomy\n",
    "    # Also determine which intergrades, if any, have no parents\n",
    "    for intergrade in intergrades:\n",
    "        parents = [i.strip() for i in intergrade.split('x')]\n",
    "        # Check if all are true\n",
    "        if all([p in issfs+forms for p in parents]):\n",
    "            intergrade_to_parents[intergrade] = parents\n",
    "        else:\n",
    "            top_level_intergrades.append(intergrade)\n",
    "\n",
    "    # Find parents of the forms, if any are in the eBird taxonomy\n",
    "    # Also determine which forms, if any, have no parents\n",
    "    for form in forms:\n",
    "        # Split the form into its individual components\n",
    "        form_parts = set(form.split(\"/\"))\n",
    "        \n",
    "        parent_issfs = []\n",
    "        for component in issfs:\n",
    "            # Split the list component into subparts and check if all are in the form_parts\n",
    "            component_parts = set(component.split(\"/\"))\n",
    "            if component_parts <= form_parts:  # Check if component_parts is a subset of form_parts\n",
    "                parent_issfs.append(component)\n",
    "        if len(parent_issfs):\n",
    "            forms_to_parents[form] = parent_issfs\n",
    "        else:\n",
    "            top_level_forms.append(form)\n",
    "        \n",
    "    return issfs, forms, intergrades, intergrade_to_parents, forms_to_parents, top_level_intergrades, top_level_forms\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Convert an (R, G, B) tuple to a hex color (#RRGGBB).\"\"\"\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(*rgb)\n",
    "\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert hex color (#RRGGBB) to an (R, G, B) tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def combine_rgb_colors(rgb_colors, fracs):\n",
    "    \"\"\"Combine a list of RGB colors proportionally.\"\"\"\n",
    "    if sum(fracs) == 0:\n",
    "        return \"#999999\"\n",
    "    else:\n",
    "        combined_rgb = tuple(\n",
    "            int(sum(frac * color[channel] for color, frac in zip(rgb_colors, fracs)))\n",
    "            for channel in range(3)\n",
    "        )\n",
    "    return rgb_to_hex(combined_rgb)\n",
    "\n",
    "def name_to_base_hue(name):\n",
    "    \"\"\"Generate a base hue from a name.\"\"\"\n",
    "    base_hue = hash(name) % 360\n",
    "    return base_hue\n",
    "\n",
    "def average_hues(hues):\n",
    "    \"\"\"Average a list of hues on the circular scale.\"\"\"\n",
    "    x = np.mean([np.cos(np.radians(h)) for h in hues])\n",
    "    y = np.mean([np.sin(np.radians(h)) for h in hues])\n",
    "    avg_hue = np.degrees(np.arctan2(y, x)) % 360\n",
    "    return avg_hue\n",
    "\n",
    "\n",
    "from colormath.color_objects import sRGBColor, LabColor\n",
    "from colormath.color_conversions import convert_color\n",
    "from colormath import color_diff_matrix\n",
    "#from colormath.color_diff import delta_e_cie2000 # deprecated and doesn't work anymore, reimplemented below\n",
    "\n",
    "def delta_e_cie2000(color1, color2, Kl=1, Kc=1, Kh=1):\n",
    "    \"\"\"\n",
    "    Calculates the Delta E (CIE2000) of two colors.\n",
    "    \"\"\"\n",
    "    def _get_lab_color1_vector(color):\n",
    "        return np.array([color.lab_l, color.lab_a, color.lab_b])\n",
    "    def _get_lab_color2_matrix(color):\n",
    "        return np.array([(color.lab_l, color.lab_a, color.lab_b)])\n",
    "\n",
    "    color1_vector = _get_lab_color1_vector(color1)\n",
    "    color2_matrix = _get_lab_color2_matrix(color2)\n",
    "    delta_e = color_diff_matrix.delta_e_cie2000(\n",
    "        color1_vector, color2_matrix, Kl=Kl, Kc=Kc, Kh=Kh)[0]\n",
    "    return delta_e\n",
    "\n",
    "def rgb_to_lab(rgb):\n",
    "    \"\"\"Convert an RGB color (0-255) to LAB color space.\"\"\"\n",
    "    srgb = sRGBColor(rgb[0] / 255, rgb[1] / 255, rgb[2] / 255, is_upscaled=False)\n",
    "    return convert_color(srgb, LabColor)\n",
    "\n",
    "def hsl_to_rgb(hue, saturation, lightness):\n",
    "    \"\"\"Convert HSL values to RGB (0-255).\"\"\"\n",
    "    r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "    return (int(r * 255), int(g * 255), int(b * 255))\n",
    "\n",
    "\n",
    "# def is_color_too_similar(hue1, hue2, threshold=40):\n",
    "#     \"\"\"Check if two hues are too similar by checking their difference in hue space.\"\"\"\n",
    "#     return abs(hue1 - hue2) < threshold or abs(hue1 - hue2) > (360 - threshold)\n",
    "\n",
    "\n",
    "def is_color_too_similar(hue1, hue2, threshold=15):\n",
    "    \"\"\"\n",
    "    Check if two hues are too similar, accounting for perceptual non-uniformity.\n",
    "    Compare using the CIEDE2000 formula in the LAB color space.\n",
    "\n",
    "    Higher threshold ==> colors need to be more different\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert hues to RGB colors using fixed saturation and lightness for comparison\n",
    "    rgb1 = hsl_to_rgb(hue1, 0.8, 0.5)  # Vivid, medium lightness\n",
    "    rgb2 = hsl_to_rgb(hue2, 0.8, 0.5)\n",
    "\n",
    "    # Convert RGB to LAB for perceptual uniformity\n",
    "    lab1 = rgb_to_lab(rgb1)\n",
    "    lab2 = rgb_to_lab(rgb2)\n",
    "\n",
    "    # Calculate perceptual difference using CIEDE2000\n",
    "    delta_e = delta_e_cie2000(lab1, lab2)\n",
    "\n",
    "    # if delta_e < threshold:\n",
    "    #     print(\"Too similar:\", hue1, hue2)\n",
    "    # else:\n",
    "    #     print(\"Not too similar:\", hue1, hue2)\n",
    "    return delta_e < threshold\n",
    "\n",
    "\n",
    "def create_distribution_adjacency_matrix(data, subspecies_cols, cell_col='cell_id'):\n",
    "    \"\"\"\n",
    "    Create an adjacency matrix based on subspecies distribution similarities.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with cells as rows and subspecies counts as columns.\n",
    "    - subspecies_cols: List of column names corresponding to subspecies counts.\n",
    "    - cell_col: Column name for cell identifiers (optional, for reference).\n",
    "\n",
    "    Returns:\n",
    "    - adjacency_matrix: A NumPy array where element [i, j] is the similarity between subspecies distributions.\n",
    "    - subspecies_list: The order of subspecies corresponding to matrix rows/columns.\n",
    "    \"\"\"\n",
    "    # Subset the subspecies columns\n",
    "    subspecies_data = data[subspecies_cols]\n",
    "\n",
    "    # Normalize each cell's counts to proportions\n",
    "    subspecies_distribution = subspecies_data.div(subspecies_data.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # Compute cosine similarity between each pair of subspecies\n",
    "    adjacency_matrix = cosine_similarity(subspecies_distribution.T)\n",
    "\n",
    "    # Return the matrix and list of subspecies\n",
    "    return adjacency_matrix, subspecies_cols\n",
    "\n",
    "\n",
    "def assign_hues(subspecies, overlap_matrix):\n",
    "    \"\"\"Assign colors to subspecies based on overlap relationships.\"\"\"\n",
    "    # Step 1: Generate base hues\n",
    "    #base_hues = {subsp: name_to_base_hue(subsp) for subsp in subspecies}\n",
    "\n",
    "    # # Step 2: Adjust hues based on overlap using graph coloring\n",
    "    # G = nx.Graph()\n",
    "    # for i, sp1 in enumerate(subspecies):\n",
    "    #     for j, sp2 in enumerate(subspecies):\n",
    "    #         if overlap_matrix[i][j] > 0.1:  # Threshold for \"overlap\"\n",
    "    #             G.add_edge(sp1, sp2)\n",
    "\n",
    "    # coloring = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "    # color_mapping = {}\n",
    "    # used_hues = []\n",
    "\n",
    "    # for subsp, color_idx in coloring.items():\n",
    "    #     # Calculate the hue with an initial base hue + an offset determined by color_idx\n",
    "    #     hue = (base_hues[subsp] + color_idx * 60) % 360  # Spread hues by 60Â° to maximize contrast\n",
    "        \n",
    "    #     # Ensure the hue is distinct from previously used hues\n",
    "    #     while any(is_color_too_similar(hue, used_hue) for used_hue in used_hues):\n",
    "    #         hue = (hue + 30) % 360  # Adjust the hue if it's too similar to previously used hues\n",
    "        \n",
    "    #     color_mapping[subsp] = hue\n",
    "    #     # Add the hue to the list of used hues\n",
    "    #     used_hues.append(hue)\n",
    "        \n",
    "    #     # # Convert the hue to RGB\n",
    "    #     # saturation, lightness = 0.8, 0.5  # Vivid, medium colors\n",
    "    #     # r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "    #     # color_mapping[subsp] = rgb_to_hex((int(r * 255), int(g * 255), int(b * 255)))\n",
    "    \n",
    "    # return color_mapping\n",
    "    return {subsp: idx*(360/(len(subspecies) + 1)) for idx, subsp in enumerate(subspecies)}\n",
    "\n",
    "def hue_to_hex_vibrant(hue):\n",
    "    saturation, lightness = 0.8, 0.5  # Vivid, medium colors\n",
    "    r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "    return rgb_to_hex((int(r * 255), int(g * 255), int(b * 255)))\n",
    "\n",
    "\n",
    "def style_function(feature, subspp_colors):\n",
    "    \"\"\"Style a cell based on the proportion of subspecies.\"\"\"\n",
    "    properties = feature['properties']\n",
    "    subspecies_values = {subsp: properties.get(subsp, 0) for subsp in subspp_colors.keys()}\n",
    "    \n",
    "    # Normalize the values to sum up to 1 for proportional allocation\n",
    "    total = sum(subspecies_values.values())\n",
    "    if total > 0:\n",
    "        fracs = [value / total for value in subspecies_values.values()]\n",
    "    else:\n",
    "        fracs = [0 for _ in subspecies_values]\n",
    "    \n",
    "    # Get RGB colors for each subspecies\n",
    "    hex_colors = [subspp_colors[subsp] for subsp in subspecies_values]\n",
    "    rgb_colors = [hex_to_rgb(color) for color in hex_colors]\n",
    "    \n",
    "    # Combine colors based on the proportional fractions\n",
    "    cell_color = combine_rgb_colors(rgb_colors, fracs)\n",
    "    \n",
    "    return {\n",
    "        'fillColor': cell_color,  # Cell color\n",
    "        'color': cell_color,  # Border color\n",
    "        'weight': 1,  # Border weight\n",
    "        'fillOpacity': 0.6,  # Cell fill transparency\n",
    "    }\n",
    "\n",
    "def calculate_overlap_intensity(overlap_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the overlap intensity for each subspecies.\n",
    "    Overlap intensity is defined as the proportion of cells that overlap with others.\n",
    "\n",
    "    Parameters:\n",
    "    - overlap_matrix: A square matrix where overlap_matrix[i][j] represents the overlap between subspecies[i] and subspecies[j].\n",
    "\n",
    "    Returns:\n",
    "    - A list of overlap intensities for each subspecies.\n",
    "    \"\"\"\n",
    "    total_overlap = np.sum(overlap_matrix, axis=1)  # Total overlap for each subspecies\n",
    "    max_overlap = np.sum(overlap_matrix, axis=1) - np.diagonal(overlap_matrix)  # Exclude self-overlap\n",
    "    return total_overlap / max_overlap\n",
    "\n",
    "\n",
    "def generate_priority_hues(subspecies, overlap_matrix):\n",
    "    \"\"\"\n",
    "    Assign perceptually distinct hues to subspecies, prioritizing those with higher overlap intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - subspecies: List of subspecies names.\n",
    "    - overlap_matrix: A square matrix where overlap_matrix[i][j] represents the overlap between subspecies[i] and subspecies[j].\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary mapping each subspecies to a distinct hue (in degrees, 0-360).\n",
    "    \"\"\"\n",
    "    n_subspecies = len(subspecies)\n",
    "\n",
    "    # Step 1: Generate perceptually distinct hues (0-360 degrees)\n",
    "    hues = np.linspace(0, 360, n_subspecies, endpoint=False)\n",
    "\n",
    "    # Step 2: Calculate overlap intensity\n",
    "    overlap_intensity = calculate_overlap_intensity(overlap_matrix)\n",
    "\n",
    "    # Step 3: Assign hues based on overlap intensity and relationships\n",
    "    G = nx.Graph()\n",
    "    for i, sp1 in enumerate(subspecies):\n",
    "        for j, sp2 in enumerate(subspecies):\n",
    "            if overlap_matrix[i][j] > 0.1:  # Add edge if overlap is above threshold\n",
    "                G.add_edge(sp1, sp2, weight=overlap_matrix[i][j])\n",
    "\n",
    "    # Sort subspecies by overlap intensity\n",
    "    subspecies_sorted = sorted(zip(subspecies, overlap_intensity), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Assign hues sequentially to prioritize highly overlapping subspecies\n",
    "    assigned_hues = {}\n",
    "    used_hues = set()\n",
    "    for subsp, _ in subspecies_sorted:\n",
    "        # Find the most distinct unused hue\n",
    "        best_hue = None\n",
    "        max_dist = -1\n",
    "        for i, hue in enumerate(hues):\n",
    "            if i in used_hues:\n",
    "                continue\n",
    "            # Check perceptual distance to already assigned hues\n",
    "            if assigned_hues:\n",
    "                dist = np.min(\n",
    "                    [min(abs(hue - assigned_hues[sp]), 360 - abs(hue - assigned_hues[sp])) for sp in assigned_hues]\n",
    "                )\n",
    "            else:\n",
    "                dist = float(\"inf\")\n",
    "            \n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                best_hue = i\n",
    "\n",
    "        # Assign the best hue\n",
    "        assigned_hues[subsp] = hues[best_hue]\n",
    "        used_hues.add(best_hue)\n",
    "    \n",
    "    return assigned_hues\n",
    "\n",
    "def get_color_mapping(sp_cell_df):\n",
    "    # Get the relationships between the infraspecies (issfs, forms, intergrades)\n",
    "    issfs, forms, intergrades, inter_to_p, form_to_p, top_inter, top_form = get_infraspecies_relationships(species, spp_dict)\n",
    "\n",
    "    # Get colors for the top-level infraspecies based on their geographic overlap\n",
    "    top_level_infras = [*issfs, *top_inter, *top_form]\n",
    "    sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "    overlap_matrix, top_level_infras = create_distribution_adjacency_matrix(sp_cell_df, top_level_infras)\n",
    "    #ssp_hues = assign_hues(top_level_infras, overlap_matrix)\n",
    "    ssp_hues = generate_priority_hues(top_level_infras, overlap_matrix)\n",
    "\n",
    "    # Get colors for the remaining species based on overlap with top-level infraspecies\n",
    "    for intergrade, parents in inter_to_p.items():\n",
    "        ssp_hues[intergrade] = average_hues([ssp_hues[parent] for parent in parents])\n",
    "    for form, parents in form_to_p.items():\n",
    "        ssp_hues[form] = average_hues([ssp_hues[parent] for parent in parents])\n",
    "\n",
    "    # Convert hues to vibrant colors\n",
    "    ssp_colors = {ssp: hue_to_hex_vibrant(hue) for ssp, hue in ssp_hues.items()}\n",
    "\n",
    "    return ssp_colors\n",
    "\n",
    "def get_bounds(geojson_result):\n",
    "    \"\"\"\n",
    "    Calculate the bounding box of all features in the GeoJSON.\n",
    "\n",
    "    Args:\n",
    "    - geojson_result: GeoJSON string with features.\n",
    "\n",
    "    Returns:\n",
    "    - Bounds as [[southwest_lat, southwest_lon], [northeast_lat, northeast_lon]].\n",
    "    \"\"\"\n",
    "    import json\n",
    "    geojson_data = json.loads(geojson_result)\n",
    "    all_coords = []\n",
    "\n",
    "    for feature in geojson_data['features']:\n",
    "        # Extract all coordinates from the polygon or multipolygon\n",
    "        coords = feature['geometry']['coordinates']\n",
    "        if feature['geometry']['type'] == \"Polygon\":\n",
    "            all_coords.extend(coords[0])  # Add outer ring of the polygon\n",
    "        elif feature['geometry']['type'] == \"MultiPolygon\":\n",
    "            for poly in coords:\n",
    "                all_coords.extend(poly[0])  # Add outer ring of each polygon\n",
    "\n",
    "    # Extract longitudes (x) and latitudes (y) correctly\n",
    "    lons, lats = zip(*all_coords)\n",
    "    return [[min(lats), min(lons)], [max(lats), max(lons)]]\n",
    "\n",
    "\n",
    "def choropleth_map(sp_cell_df, common_name, subspp_colors):\n",
    "    \"\"\"Creates a choropleth map given species data.\"\"\"\n",
    "    \n",
    "    f = folium.Figure()\n",
    "    map = folium.Map(location=[47, -122], zoom_start=5, tiles=\"cartodbpositron\")\n",
    "    f.add_child(map)\n",
    "\n",
    "    sp = sp_cell_df.columns[0]\n",
    "    subspp = sp_cell_df.columns[1:]\n",
    "    \n",
    "    list_features = []\n",
    "    for _, row in sp_cell_df.iterrows():\n",
    "        #percentages = (row[subspp] / row[sp]) # For the previous implementation that colored the map by % of total sightings instead of % of ssp sightings\n",
    "        percentages = (row[subspp] / sum(row[subspp]))*100\n",
    "        percentages_dict = percentages.to_dict()\n",
    "        \n",
    "        # Precompute tooltip text showing only non-zero percentages\n",
    "        percentages_dict_ordered = pd.DataFrame(percentages_dict, index=['pct']).T.query('pct > 0')['pct'].sort_values(ascending=False).to_dict()\n",
    "\n",
    "        tooltip_text = []\n",
    "        for subsp, percent in percentages_dict_ordered.items():\n",
    "            tooltip_text.append(f\"{subsp}: {percent:.0f}%\")\n",
    "        \n",
    "        # Add tooltip as a string to the properties\n",
    "        percentages_dict[\"tooltip\"] = \"<br>\".join(tooltip_text) if tooltip_text else \"No data\"\n",
    "\n",
    "        geometry_for_row = h3.cells_to_geo(cells=[row.name])\n",
    "        feature = Feature(\n",
    "            geometry=geometry_for_row,\n",
    "            id=row.name,\n",
    "            properties=percentages_dict)\n",
    "        list_features.append(feature)\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "    \n",
    "    # Add GeoJSON layer to the map\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: style_function(feature, subspp_colors),\n",
    "        name=f'{sp} Subspecies Map'\n",
    "    ).add_to(map)\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: {\n",
    "            'weight': 0,  # No border weight\n",
    "            'color': 'transparent',  # No border color\n",
    "            'fillOpacity': 0.6  # Fill transparency\n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            #fields=list(subspp),\n",
    "            #aliases=[subsp[len(sp)+1:] for subsp in subspp], # Removes ssp name\n",
    "            fields=[\"tooltip\"],\n",
    "            aliases=[\"Reported\\nSubspecies\"],\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            labels=True,\n",
    "            labels_format=\"{:.2f}%\",\n",
    "            #highlight_function=lambda x: x.update({'text': [f'{k}: {v:.2f}%' for k, v in x['properties'].items() if v > 0]})\n",
    "        )\n",
    "    ).add_to(map)\n",
    "\n",
    "\n",
    "    # Add legend\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"position: fixed; top: 10px; right: 10px; width: 150px; height: auto; z-index: 9999; background-color: white; box-shadow: 0 0 5px rgba(0, 0, 0, 0.2); border: 1px solid lightgray; border-radius: 5px; padding: 10px; font-size: 10px;\">\n",
    "        <strong>{common_name} infraspecies</strong><br>\n",
    "    \"\"\"\n",
    "    for subsp, color in subspp_colors.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <span style=\"display: inline-block; width: 20px; height: 10px; margin-right: 10px; background-color: {color};\"></span>\n",
    "            {subsp}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    legend_html += \"</div>\"\n",
    "    legend_element = folium.Element(legend_html)\n",
    "    map.get_root().html.add_child(legend_element)\n",
    "\n",
    "    # Calculate bounds and adjust the map's view\n",
    "    bounds = get_bounds(geojson_result)\n",
    "    map.fit_bounds(bounds)\n",
    "\n",
    "    return map\n",
    "\n",
    "# Example usage:\n",
    "#for sp_code in sp_codes:\n",
    "remake_maps = True\n",
    "for sp_code in sp_codes:\n",
    "    print(\"Mapping\", sp_code)\n",
    "    common_name = taxonomy[taxonomy['SPECIES_CODE'] == sp_code].PRIMARY_COM_NAME.values[0]\n",
    "    for resolution in [2,3,4,5]:\n",
    "        species = taxonomy[taxonomy['PRIMARY_COM_NAME'] == common_name].SCI_NAME.values[0]\n",
    "        dataname = f\"sp_cell_dfs/{species.replace(' ', '-')}_resolution{resolution}.csv\"\n",
    "        if not Path(dataname).exists():\n",
    "            print(\"No data for\", species, \"at resolution\", resolution)\n",
    "            continue\n",
    "        map_filename = f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\"\n",
    "        if Path(map_filename).exists() and not remake_maps:\n",
    "            print(\"Map already exists for\", species, \"at resolution\", resolution)\n",
    "            continue\n",
    "        sp_cell_df = pd.read_csv(dataname, index_col=0)\n",
    "        sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "        subspecies = sp_cell_df.columns[1:]\n",
    "        if resolution == 2:\n",
    "            subspp_colors = get_color_mapping(sp_cell_df)\n",
    "            \n",
    "        m = choropleth_map(sp_cell_df, common_name, subspp_colors)\n",
    "        m.save(f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CSV of map URLs for the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maps/Butorides-striata_4.html\n",
      "maps/Butorides-striata_5.html\n",
      "maps/Butorides-striata_2.html\n",
      "maps/Butorides-striata_3.html\n",
      "maps/Branta-bernicla_3.html\n",
      "maps/Setophaga-coronata_3.html\n",
      "maps/Buteo-jamaicensis_4.html\n",
      "maps/Zonotrichia-leucophrys_3.html\n",
      "maps/Sturnella-magna_5.html\n",
      "maps/Loxia-curvirostra_3.html\n",
      "maps/Garrulus-glandarius_4.html\n",
      "maps/Garrulus-glandarius_5.html\n",
      "maps/Loxia-curvirostra_2.html\n",
      "maps/Sturnella-magna_4.html\n",
      "maps/Zonotrichia-leucophrys_2.html\n",
      "maps/Buteo-jamaicensis_5.html\n",
      "maps/Setophaga-coronata_2.html\n",
      "maps/Branta-bernicla_2.html\n",
      "maps/Sturnella-magna_3.html\n",
      "maps/Loxia-curvirostra_5.html\n",
      "maps/Garrulus-glandarius_2.html\n",
      "maps/Branta-bernicla_5.html\n",
      "maps/Setophaga-coronata_5.html\n",
      "maps/Buteo-jamaicensis_2.html\n",
      "maps/Branta-hutchinsii_2.html\n",
      "maps/Zonotrichia-leucophrys_5.html\n",
      "maps/Zonotrichia-leucophrys_4.html\n",
      "maps/Buteo-jamaicensis_3.html\n",
      "maps/Setophaga-coronata_4.html\n",
      "maps/Branta-bernicla_4.html\n",
      "maps/Garrulus-glandarius_3.html\n",
      "maps/Loxia-curvirostra_4.html\n",
      "maps/Sturnella-magna_2.html\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"common_name\", \"scientific_name\", \"resolution\", \"map_url\"])\n",
    "maps_dir = Path(\"docs/maps\")\n",
    "for idx, file in enumerate(maps_dir.glob(\"*.html\")):\n",
    "    resolution = file.stem.split(\"_\")[-1]\n",
    "    species = file.stem.replace(f\"_{resolution}\", \"\")\n",
    "    common_name = taxonomy[taxonomy['SCI_NAME'] == species.replace('-', ' ')].PRIMARY_COM_NAME.values[0]\n",
    "    map_url = Path(Path(file).parent.stem).joinpath(Path(file).name)\n",
    "    print(map_url)\n",
    "    df.loc[idx] = [common_name, species, resolution, map_url]\n",
    "df.to_csv(\"docs/data/map_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
