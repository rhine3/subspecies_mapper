{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import h3\n",
    "import folium\n",
    "from geojson import Feature, FeatureCollection\n",
    "import _json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "pd.options.mode.copy_on_write = True \n",
    "from functools import reduce\n",
    "from folium import GeoJsonTooltip\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON of subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAXON_ORDER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>TAXON_CONCEPT_ID</th>\n",
       "      <th>PRIMARY_COM_NAME</th>\n",
       "      <th>SCI_NAME</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>SPECIES_GROUP</th>\n",
       "      <th>REPORT_AS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common Ostrich</td>\n",
       "      <td>Struthio camelus</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Somali Ostrich</td>\n",
       "      <td>Struthio molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>slash</td>\n",
       "      <td>y00934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common/Somali Ostrich</td>\n",
       "      <td>Struthio camelus/molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>species</td>\n",
       "      <td>soucas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southern Cassowary</td>\n",
       "      <td>Casuarius casuarius</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>species</td>\n",
       "      <td>dwacas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dwarf Cassowary</td>\n",
       "      <td>Casuarius bennetti</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TAXON_ORDER CATEGORY SPECIES_CODE  TAXON_CONCEPT_ID       PRIMARY_COM_NAME  \\\n",
       "0            2  species      ostric2               NaN         Common Ostrich   \n",
       "1            7  species      ostric3               NaN         Somali Ostrich   \n",
       "2            8    slash       y00934               NaN  Common/Somali Ostrich   \n",
       "3           10  species      soucas1               NaN     Southern Cassowary   \n",
       "4           11  species      dwacas1               NaN        Dwarf Cassowary   \n",
       "\n",
       "                         SCI_NAME             ORDER  \\\n",
       "0                Struthio camelus  Struthioniformes   \n",
       "1          Struthio molybdophanes  Struthioniformes   \n",
       "2  Struthio camelus/molybdophanes  Struthioniformes   \n",
       "3             Casuarius casuarius    Casuariiformes   \n",
       "4              Casuarius bennetti    Casuariiformes   \n",
       "\n",
       "                              FAMILY        SPECIES_GROUP REPORT_AS  \n",
       "0          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "1          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "2          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "3  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "4  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load eBird taxonomy\n",
    "taxonomy = pd.read_csv(\"eBird_taxonomy_v2024.csv\")\n",
    "taxonomy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['species', 'slash', 'issf', 'hybrid', 'spuh', 'domestic', 'form',\n",
       "       'intergrade'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What categories are there in the taxonomy?\n",
    "taxonomy.CATEGORY.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11145"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many species are there?\n",
    "species = taxonomy[taxonomy.CATEGORY == 'species']\n",
    "len(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3843"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many infraspecific entries are there?\n",
    "infrasp_categories = ['issf', 'form', 'intergrade']\n",
    "infraspp = taxonomy[taxonomy.CATEGORY.isin(infrasp_categories)]\n",
    "len(infraspp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  cell1 cell1.1\n",
      "0  2  cell2   cell2\n",
      "1  3  cell3   cell3\n",
      "   1  cell1 cell1.1\n",
      "2  4  cell4   cell4\n",
      "3  5  cell5   cell5\n",
      "   1  cell1 cell1.1\n",
      "4  6  cell6   cell6\n"
     ]
    }
   ],
   "source": [
    "start_row=0\n",
    "end_row=2\n",
    "for x in pd.read_csv(\"example.csv\", header=0, skiprows=range(0,2), chunksize=2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping species to their infraspecies, by category\n",
    "# Most species have a single category of infraspecies (e.g. either form or subspecies, not both)\n",
    "# However some species have infraspecies in multiple categories, e.g. Brant (Branta bernicla) has both subspecies and forms\n",
    "spp_dict = species[['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "# Add infraspecies to spp_json\n",
    "for sp in tqdm(spp_dict.keys()):\n",
    "    # Get infraspecies for this species\n",
    "    infraspp_for_sp = infraspp[infraspp['SCI_NAME'].apply(lambda x: x[:len(sp)] == sp)]\n",
    "    infraspp_dict = dict()\n",
    "\n",
    "    # Add infraspecies to spp_json by category\n",
    "    for cat in infrasp_categories:\n",
    "        infrasp_in_category = infraspp_for_sp[infraspp_for_sp.CATEGORY == cat]\n",
    "        infrasp_cat_dict = infrasp_in_category[\n",
    "            ['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "        if len(infrasp_cat_dict.keys()) > 0:\n",
    "            infraspp_dict[cat] = infrasp_cat_dict\n",
    "    spp_dict[sp]['infraspecies'] = infraspp_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"infraspecies_ebird.json\", 'w') as f:\n",
    "    f.write(json.dumps(spp_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"infraspecies_ebird.json\") as f:\n",
    "    spp_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep eBird data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following H3 resolutions:\n",
    "```\n",
    "Res\tAverage Hexagon Area (km2)\tPentagon Area* (km2)\tRatio (P/H)\n",
    "2\t86,801.780398997\t44,930.898497879\t0.5176\n",
    "3\t12,393.434655088\t6,315.472267516\t0.5096\n",
    "4\t1,770.347654491\t896.582383141\t0.5064\n",
    "```\n",
    "\n",
    "They have this many cells:\n",
    "```\n",
    "Res\tTotal number of cells\tNumber of hexagons\tNumber of pentagons\n",
    "2\t5,882\t5,870\t12\n",
    "3\t41,162\t41,150\t12\n",
    "4\t288,122\t288,110\t12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of species codes based on the data downloaded from eBird\n",
    "sp_codes = [x.name.split('_')[1] for x in list(Path(\"data/\").glob(\"*.zip\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine number of sightings of each subspecies per grid cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing strher\n",
      "No more data to process, total rows in dataset:  499586\n",
      "\n",
      "\n",
      "\n",
      "Processing easmea\n",
      "No more data to process, total rows in dataset:  2400001\n",
      "\n",
      "\n",
      "\n",
      "Processing yerwar\n",
      "No more data to process, total rows in dataset:  12200001\n",
      "\n",
      "\n",
      "\n",
      "Processing eurjay1\n",
      "No more data to process, total rows in dataset:  1200001\n",
      "\n",
      "\n",
      "\n",
      "Processing brant\n",
      "No more data to process, total rows in dataset:  800001\n",
      "\n",
      "\n",
      "\n",
      "Processing whcspa\n",
      "No more data to process, total rows in dataset:  6400001\n",
      "\n",
      "\n",
      "\n",
      "Processing cacgoo1\n",
      "No more data to process, total rows in dataset:  800001\n",
      "\n",
      "\n",
      "\n",
      "Processing horlar\n",
      "No more data to process, total rows in dataset:  1900001\n",
      "\n",
      "\n",
      "\n",
      "Processing coatit2\n",
      "No more data to process, total rows in dataset:  800001\n",
      "\n",
      "\n",
      "\n",
      "Processing foxspa\n",
      "No more data to process, total rows in dataset:  1900001\n",
      "\n",
      "\n",
      "\n",
      "Processing daejun\n",
      "No more data to process, total rows in dataset:  14296832\n",
      "\n",
      "\n",
      "\n",
      "Processing orcwar\n",
      "No more data to process, total rows in dataset:  2900001\n",
      "\n",
      "\n",
      "\n",
      "Processing yebcha\n",
      "No more data to process, total rows in dataset:  1000001\n",
      "\n",
      "\n",
      "\n",
      "Processing cangoo\n",
      "No more data to process, total rows in dataset:  20300001\n",
      "\n",
      "\n",
      "\n",
      "Processing whiwag\n",
      "No more data to process, total rows in dataset:  2800001\n",
      "\n",
      "\n",
      "\n",
      "Processing norfli\n",
      "No more data to process, total rows in dataset:  14700001\n",
      "\n",
      "\n",
      "\n",
      "Processing rethaw\n",
      "No more data to process, total rows in dataset:  11778727\n",
      "\n",
      "\n",
      "\n",
      "Processing comeid\n",
      "No more data to process, total rows in dataset:  1072050\n",
      "\n",
      "\n",
      "\n",
      "Processing perfal\n",
      "No more data to process, total rows in dataset:  1759264\n",
      "\n",
      "\n",
      "\n",
      "Processing redcro\n",
      "No more data to process, total rows in dataset:  797057\n"
     ]
    }
   ],
   "source": [
    "def clean_ebd(\n",
    "    full_df,\n",
    "    remove_unconfirmed=True, \n",
    "    remove_reviewed=False,\n",
    "    resolutions = resolutions,\n",
    "):\n",
    "\n",
    "    # Remove duplicate checklists\n",
    "    full_df = full_df[full_df['GROUP IDENTIFIER'].isnull() | ~full_df[full_df['GROUP IDENTIFIER'].notnull()].duplicated(subset=[\"GROUP IDENTIFIER\", \"SCIENTIFIC NAME\"],keep='first')]\n",
    "\n",
    "    # Removed unconfirmed observations or reviewed observations, if desired\n",
    "    if remove_unconfirmed:\n",
    "        full_df = full_df[full_df[\"APPROVED\"] == 1]\n",
    "    if remove_reviewed:\n",
    "        full_df = full_df[full_df[\"REVIEWED\"] == 0]\n",
    "\n",
    "    # Convert latitude and longitude to an H3 hexagon ID\n",
    "    for resolution in resolutions:\n",
    "        full_df[f'hex_id_{resolution}'] = full_df.apply(lambda row:  h3.latlng_to_cell(row.LATITUDE, row.LONGITUDE, resolution), axis=1)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "def get_grid_cell_species_data(cell_df, sp, subspp, resolution):\n",
    "    \"\"\"Get # of checklists containing a species and each subspecies\n",
    "\n",
    "    Args:\n",
    "    - cell_df: pd.DataFrame, dataframe of data for a single grid cell (1 row per observation)\n",
    "    - sp: str, scientific name of species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "\n",
    "    Returns:\n",
    "    - cell_data: dict, with keys 'cell_id', species name, and subspecies names\n",
    "    \"\"\"\n",
    "    # Total number of checklists containing the species\n",
    "    num_checklists = cell_df[\"SAMPLING EVENT IDENTIFIER\"].nunique()\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_data = {'cell_id': cell_df[f\"hex_id_{resolution}\"].iloc[0]}\n",
    "    cell_data[sp] = num_checklists\n",
    "\n",
    "    # Add number of checklists containing each subspecies\n",
    "    for subsp in subspp:\n",
    "        num_subsp = cell_df[cell_df[\"SUBSPECIES SCIENTIFIC NAME\"] == subsp].shape[0]\n",
    "        cell_data[subsp] = num_subsp\n",
    "\n",
    "    return cell_data\n",
    "\n",
    "\n",
    "def get_species_df(sp, sp_df, subspp, resolution):\n",
    "    \"\"\"Make dataframe of species & subspecies data for every cell for a given species\n",
    "\n",
    "    Args:\n",
    "    - sp: str, scientific name of species\n",
    "    - df: pd.DataFrame, dataframe of data for this species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "    - resolution: int, H3 resolution level\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_dicts = []\n",
    "    for cell in sp_df[f\"hex_id_{resolution}\"].unique():\n",
    "        cell_df = sp_df[sp_df[f\"hex_id_{resolution}\"] == cell]\n",
    "        cell_data = get_grid_cell_species_data(cell_df, sp, subspp, resolution)\n",
    "        cell_dicts.append(cell_data)\n",
    "\n",
    "    sp_cell_df = pd.DataFrame(cell_dicts, index=range(len(cell_dicts)))\n",
    "    sp_cell_df.set_index(\"cell_id\", inplace=True)\n",
    "\n",
    "    return sp_cell_df\n",
    "\n",
    "\n",
    "# The dictionary we've created of all species and their infraspecies, by category\n",
    "spp_dict = json.load(open(\"infraspecies_ebird.json\"))\n",
    "\n",
    "# Only include the columns we're using\n",
    "use_cols = [\n",
    "        'SCIENTIFIC NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "        'SAMPLING EVENT IDENTIFIER', 'GROUP IDENTIFIER',\n",
    "        'LATITUDE', 'LONGITUDE', \n",
    "        'REVIEWED', 'APPROVED', 'OBSERVATION DATE']\n",
    "# These were used in an earlier version of the code, but are not needed now\n",
    "# use_cols = [\n",
    "#         'CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', \n",
    "#         'SCIENTIFIC NAME','SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "#         'SAMPLING EVENT IDENTIFIER',\n",
    "#         'LATITUDE', 'LONGITUDE', 'REVIEWED', 'APPROVED', 'GROUP IDENTIFIER', 'OBSERVATION DATE']\n",
    "\n",
    "# Process each species's dataset\n",
    "for sp_code in sp_codes:\n",
    "    print(\"\\n\\n\\nProcessing\", sp_code)\n",
    "    dataset_filepath = f\"data/ebd_{sp_code}_relOct-2024/ebd_{sp_code}_relOct-2024.txt\"\n",
    "\n",
    "    # Where we will store each batch of data\n",
    "    ssp_batch_directory = Path('batches/')\n",
    "    ssp_batch_directory.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read in CSV in batches\n",
    "    chunk_rows = 100000 # May want to increase this number after using FireDuck\n",
    "\n",
    "    # Keep track of the progress made in each batch using a tracker CSV\n",
    "    tracker_filepath = f\"progress_trackers/{sp_code}_tracker_rowsperchunk-{chunk_rows}.csv\"\n",
    "\n",
    "    # If already started processing this species, pick up where we left off \n",
    "    if Path(tracker_filepath).exists():\n",
    "        tracker = pd.read_csv(tracker_filepath)\n",
    "        tracker[\"spp_to_do\"] = tracker[\"spp_to_do\"].apply(ast.literal_eval) \n",
    "        tracker[\"spp_done\"] = tracker[\"spp_done\"].apply(ast.literal_eval) \n",
    "        start_idx = tracker.index[-1]\n",
    "        spp_to_do = set(tracker.loc[start_idx].spp_to_do) - set(tracker.loc[start_idx].spp_done)\n",
    "        if spp_to_do == set():\n",
    "            prev_end_row = tracker.loc[start_idx].end_row\n",
    "            prev_start_row = tracker.loc[start_idx].start_row\n",
    "            if prev_end_row - prev_start_row < chunk_rows:\n",
    "                print(\"No more data to process, total rows in dataset: \", prev_end_row)\n",
    "                continue\n",
    "            skiprows = tracker.loc[start_idx].end_row # This is the row after the last one we processed\n",
    "            start_idx = start_idx + 1\n",
    "            spp_to_do = None\n",
    "        else:\n",
    "            skiprows = tracker.loc[start_idx].start_row\n",
    "\n",
    "    else:\n",
    "        tracker = pd.DataFrame(columns=[\"start_row\", \"end_row\", \"spp_to_do\", \"spp_done\"])\n",
    "        start_idx = 0\n",
    "        spp_to_do = None\n",
    "        skiprows=0\n",
    "\n",
    "    for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=10, header=0, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n",
    "        print(f\"Processing chunk {idx}...\")\n",
    "        print(\"Start\", (start_idx + idx)*chunk_rows, \"End\", (start_idx + idx)*chunk_rows+chunk_rows)\n",
    "        if chunk.shape[0] == 0:\n",
    "            print(\"No more data to process, total rows in dataset: \", (start_idx + idx)*chunk_rows)\n",
    "            break\n",
    "        if chunk.shape[0] < chunk_rows:\n",
    "            end_row = (start_idx+idx)*chunk_rows + chunk.shape[0]+1\n",
    "            print(f\"Last chunk, total rows in dataset:\", end_row)\n",
    "        else:\n",
    "            end_row = (start_idx + idx)*chunk_rows+chunk_rows\n",
    "        cleaned = clean_ebd(chunk)\n",
    "        if spp_to_do == None: # Add new row\n",
    "            spp_to_do = list(set(cleaned[\"SCIENTIFIC NAME\"].unique()))\n",
    "            tracker.loc[start_idx+idx] = [(start_idx + idx)*chunk_rows, end_row, spp_to_do, []]\n",
    "\n",
    "        \n",
    "        for sp in spp_to_do:\n",
    "            cleaned_sp = cleaned[cleaned[\"SCIENTIFIC NAME\"] == sp]\n",
    "            if cleaned_sp.shape[0] == 0:\n",
    "                #print(f\"No data for {sp}\")\n",
    "                continue\n",
    "\n",
    "            # Get list of subspecies\n",
    "            subspp = []\n",
    "            for k, val in spp_dict[sp]['infraspecies'].items():\n",
    "                subspp.extend(val.keys())\n",
    "            \n",
    "            # Get data on presence of each subspp for each resolution\n",
    "            for resolution in resolutions:\n",
    "                species_df = get_species_df(sp, cleaned_sp, subspp, resolution)\n",
    "                filename = ssp_batch_directory.joinpath(f'{sp}_row{(start_idx+idx)*chunk_rows}-{end_row}_resolution{resolution}.csv')\n",
    "                species_df.to_csv(filename)\n",
    "\n",
    "            tracker.loc[start_idx+idx].spp_done += [sp]\n",
    "            tracker.to_csv(tracker_filepath, index=False)\n",
    "\n",
    "        spp_to_do = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum up the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cell_df_directory = Path('sp_cell_dfs/')\n",
    "sp_cell_df_directory.mkdir(exist_ok=True)\n",
    "\n",
    "def parse_batch_files(ssp_batch_directory):\n",
    "    batch_files = list(ssp_batch_directory.glob(\"*.csv\"))\n",
    "    file_info = [n.name.split(\"_\") for n in batch_files]\n",
    "    files = pd.DataFrame(file_info, columns=['SCIENTIFIC NAME', 'ROW RANGE', 'RESOLUTION'])\n",
    "    files['FILENAME'] = batch_files\n",
    "    for (species, resolution), species_df in files.groupby([\"SCIENTIFIC NAME\", 'RESOLUTION']):\n",
    "        species = species.replace(\" \", \"-\")\n",
    "        resolution = resolution[:-4]\n",
    "        all_dataframes = [pd.read_csv(f, index_col=0) for f in species_df.FILENAME] \n",
    "        sp_cell_df = reduce(lambda a, b: a.add(b, fill_value=0), all_dataframes)\n",
    "        filename = sp_cell_df_directory.joinpath(f'{species}_{resolution}.csv')\n",
    "        sp_cell_df.to_csv(filename)\n",
    "\n",
    "\n",
    "\n",
    "parse_batch_files(ssp_batch_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, LineString, mapping\n",
    "from shapely.ops import split\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import colorsys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from colormath.color_objects import sRGBColor, LabColor\n",
    "from colormath.color_conversions import convert_color\n",
    "from colormath import color_diff_matrix\n",
    "#from colormath.color_diff import delta_e_cie2000 # deprecated and doesn't work anymore, reimplemented below\n",
    "\n",
    "def get_infraspecies_relationships(sp, spp_dict=spp_dict):\n",
    "    data = spp_dict[sp]['infraspecies']\n",
    "    \n",
    "    # Get a list of each type of infraspecies\n",
    "    if \"issf\" in data.keys():\n",
    "        issfs = [k.replace(sp+' ', '') for k in data[\"issf\"].keys()] # Recognized ssp or ssp groups\n",
    "    else:\n",
    "        issfs = []\n",
    "    if \"form\" in data.keys():\n",
    "        forms = [k.replace(sp+' ', '') for k in data[\"form\"].keys()] # Forms\n",
    "    else:\n",
    "        forms = []\n",
    "    if \"intergrade\" in data.keys():\n",
    "        intergrades = [k.replace(sp+' ', '') for k in data[\"intergrade\"].keys()] # Intergrades (between ssp? forms?)\n",
    "    else:\n",
    "        intergrades = []\n",
    "\n",
    "    intergrade_to_parents = dict()\n",
    "    forms_to_parents = dict()\n",
    "    top_level_intergrades = []\n",
    "    top_level_forms = []\n",
    "\n",
    "    # Find parents of the intergrades, if any are in the eBird taxonomy\n",
    "    # Also determine which intergrades, if any, have no parents\n",
    "    for intergrade in intergrades:\n",
    "        parents = [i.strip() for i in intergrade.split('x')]\n",
    "        # Check if all are true\n",
    "        if all([p in issfs+forms for p in parents]):\n",
    "            intergrade_to_parents[intergrade] = parents\n",
    "        else:\n",
    "            top_level_intergrades.append(intergrade)\n",
    "\n",
    "    # Find parents of the forms, if any are in the eBird taxonomy\n",
    "    # Also determine which forms, if any, have no parents\n",
    "    for form in forms:\n",
    "        # Split the form into its individual components\n",
    "        form_parts = set(form.split(\"/\"))\n",
    "        \n",
    "        parent_issfs = []\n",
    "        for component in issfs:\n",
    "            # Split the list component into subparts and check if all are in the form_parts\n",
    "            component_parts = set(component.split(\"/\"))\n",
    "            if component_parts <= form_parts:  # Check if component_parts is a subset of form_parts\n",
    "                parent_issfs.append(component)\n",
    "        if len(parent_issfs):\n",
    "            forms_to_parents[form] = parent_issfs\n",
    "        else:\n",
    "            top_level_forms.append(form)\n",
    "        \n",
    "    return issfs, forms, intergrades, intergrade_to_parents, forms_to_parents, top_level_intergrades, top_level_forms\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Convert an (R, G, B) tuple to a hex color (#RRGGBB).\"\"\"\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(*rgb)\n",
    "\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert hex color (#RRGGBB) to an (R, G, B) tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def combine_rgb_colors(rgb_colors, fracs):\n",
    "    \"\"\"Combine a list of RGB colors proportionally.\"\"\"\n",
    "    if sum(fracs) == 0:\n",
    "        return \"#999999\"\n",
    "    else:\n",
    "        combined_rgb = tuple(\n",
    "            int(sum(frac * color[channel] for color, frac in zip(rgb_colors, fracs)))\n",
    "            for channel in range(3)\n",
    "        )\n",
    "    return rgb_to_hex(combined_rgb)\n",
    "\n",
    "def name_to_base_hue(name):\n",
    "    \"\"\"Generate a base hue from a name.\"\"\"\n",
    "    base_hue = hash(name) % 360\n",
    "    return base_hue\n",
    "\n",
    "def average_hues(hues):\n",
    "    \"\"\"Average a list of hues on the circular scale.\"\"\"\n",
    "    x = np.mean([np.cos(np.radians(h)) for h in hues])\n",
    "    y = np.mean([np.sin(np.radians(h)) for h in hues])\n",
    "    avg_hue = np.degrees(np.arctan2(y, x)) % 360\n",
    "    return avg_hue\n",
    "\n",
    "\n",
    "def delta_e_cie2000(color1, color2, Kl=1, Kc=1, Kh=1):\n",
    "    \"\"\"\n",
    "    Calculates the Delta E (CIE2000) of two colors.\n",
    "    \"\"\"\n",
    "    def _get_lab_color1_vector(color):\n",
    "        return np.array([color.lab_l, color.lab_a, color.lab_b])\n",
    "    def _get_lab_color2_matrix(color):\n",
    "        return np.array([(color.lab_l, color.lab_a, color.lab_b)])\n",
    "\n",
    "    color1_vector = _get_lab_color1_vector(color1)\n",
    "    color2_matrix = _get_lab_color2_matrix(color2)\n",
    "    delta_e = color_diff_matrix.delta_e_cie2000(\n",
    "        color1_vector, color2_matrix, Kl=Kl, Kc=Kc, Kh=Kh)[0]\n",
    "    return delta_e\n",
    "\n",
    "def rgb_to_lab(rgb):\n",
    "    \"\"\"Convert an RGB color (0-255) to LAB color space.\"\"\"\n",
    "    srgb = sRGBColor(rgb[0] / 255, rgb[1] / 255, rgb[2] / 255, is_upscaled=False)\n",
    "    return convert_color(srgb, LabColor)\n",
    "\n",
    "def hsl_to_rgb(hue, saturation, lightness):\n",
    "    \"\"\"Convert HSL values to RGB (0-255).\"\"\"\n",
    "    r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "    return (int(r * 255), int(g * 255), int(b * 255))\n",
    "\n",
    "\n",
    "def is_color_too_similar(hue1, hue2, threshold=15):\n",
    "    \"\"\"\n",
    "    Check if two hues are too similar, accounting for perceptual non-uniformity.\n",
    "    Compare using the CIEDE2000 formula in the LAB color space.\n",
    "\n",
    "    Higher threshold ==> colors need to be more different\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert hues to RGB colors using fixed saturation and lightness for comparison\n",
    "    rgb1 = hsl_to_rgb(hue1, 0.8, 0.5)  # Vivid, medium lightness\n",
    "    rgb2 = hsl_to_rgb(hue2, 0.8, 0.5)\n",
    "\n",
    "    # Convert RGB to LAB for perceptual uniformity\n",
    "    lab1 = rgb_to_lab(rgb1)\n",
    "    lab2 = rgb_to_lab(rgb2)\n",
    "\n",
    "    # Calculate perceptual difference using CIEDE2000\n",
    "    delta_e = delta_e_cie2000(lab1, lab2)\n",
    "    return delta_e < threshold\n",
    "\n",
    "\n",
    "def create_distribution_adjacency_matrix(data, subspecies_cols, cell_col='cell_id'):\n",
    "    \"\"\"\n",
    "    Create an adjacency matrix based on subspecies distribution similarities.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with cells as rows and subspecies counts as columns.\n",
    "    - subspecies_cols: List of column names corresponding to subspecies counts.\n",
    "    - cell_col: Column name for cell identifiers (optional, for reference).\n",
    "\n",
    "    Returns:\n",
    "    - adjacency_matrix: A NumPy array where element [i, j] is the similarity between subspecies distributions.\n",
    "    - subspecies_list: The order of subspecies corresponding to matrix rows/columns.\n",
    "    \"\"\"\n",
    "    # Subset the subspecies columns\n",
    "    subspecies_data = data[subspecies_cols]\n",
    "\n",
    "    # Normalize each cell's counts to proportions\n",
    "    subspecies_distribution = subspecies_data.div(subspecies_data.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # Compute cosine similarity between each pair of subspecies\n",
    "    adjacency_matrix = cosine_similarity(subspecies_distribution.T)\n",
    "\n",
    "    # Return the matrix and list of subspecies\n",
    "    return adjacency_matrix, subspecies_cols\n",
    "\n",
    "\n",
    "def hue_to_hex_vibrant(hue):\n",
    "    saturation, lightness = 0.8, 0.5  # Vivid, medium colors\n",
    "    r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "    return rgb_to_hex((int(r * 255), int(g * 255), int(b * 255)))\n",
    "\n",
    "\n",
    "def style_function(feature, subspp_colors):\n",
    "    \"\"\"Style a cell based on the proportion of subspecies.\"\"\"\n",
    "    properties = feature['properties']\n",
    "    subspecies_values = dict()\n",
    "    for subsp in subspp_colors.keys():\n",
    "        val = properties.get(subsp, 0)\n",
    "        if val > 0:\n",
    "            subspecies_values[subsp] = val\n",
    "    \n",
    "    # Normalize the values to sum up to 1 for proportional allocation\n",
    "    total = sum(subspecies_values.values())\n",
    "    if total > 0:\n",
    "        fracs = [value / total for value in subspecies_values.values()]\n",
    "    else:\n",
    "        fracs = [0 for _ in subspecies_values]\n",
    "    \n",
    "    # Get RGB colors for each subspecies\n",
    "    hex_colors = [subspp_colors[subsp] for subsp in subspecies_values]\n",
    "    rgb_colors = [hex_to_rgb(color) for color in hex_colors]\n",
    "    \n",
    "    # Combine colors based on the proportional fractions\n",
    "    cell_color = combine_rgb_colors(rgb_colors, fracs)\n",
    "    \n",
    "    return {\n",
    "        'fillColor': cell_color,  # Cell color\n",
    "        'color': cell_color,  # Border color\n",
    "        'weight': 1,  # Border weight\n",
    "        'fillOpacity': 0.6,  # Cell fill transparency\n",
    "    }\n",
    "\n",
    "def calculate_overlap_intensity(overlap_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the overlap intensity for each subspecies.\n",
    "    Overlap intensity is defined as the proportion of cells that overlap with others.\n",
    "\n",
    "    Parameters:\n",
    "    - overlap_matrix: A square matrix where overlap_matrix[i][j] represents the overlap between subspecies[i] and subspecies[j].\n",
    "\n",
    "    Returns:\n",
    "    - A list of overlap intensities for each subspecies.\n",
    "    \"\"\"\n",
    "    total_overlap = np.sum(overlap_matrix, axis=1)  # Total overlap for each subspecies\n",
    "    max_overlap = np.sum(overlap_matrix, axis=1) - np.diagonal(overlap_matrix)  # Exclude self-overlap\n",
    "    return total_overlap / max_overlap\n",
    "\n",
    "\n",
    "def generate_priority_hues(subspecies, overlap_matrix):\n",
    "    \"\"\"\n",
    "    Assign perceptually distinct hues to subspecies, prioritizing those with higher overlap intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - subspecies: List of subspecies names.\n",
    "    - overlap_matrix: A square matrix where overlap_matrix[i][j] represents the overlap between subspecies[i] and subspecies[j].\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary mapping each subspecies to a distinct hue (in degrees, 0-360).\n",
    "    \"\"\"\n",
    "    n_subspecies = len(subspecies)\n",
    "\n",
    "    # Step 1: Generate perceptually distinct hues (0-360 degrees)\n",
    "    hues = np.linspace(0, 360, n_subspecies, endpoint=False)\n",
    "\n",
    "    # Step 2: Calculate overlap intensity\n",
    "    overlap_intensity = calculate_overlap_intensity(overlap_matrix)\n",
    "\n",
    "    # Step 3: Assign hues based on overlap intensity and relationships\n",
    "    G = nx.Graph()\n",
    "    for i, sp1 in enumerate(subspecies):\n",
    "        for j, sp2 in enumerate(subspecies):\n",
    "            if overlap_matrix[i][j] > 0.1:  # Add edge if overlap is above threshold\n",
    "                G.add_edge(sp1, sp2, weight=overlap_matrix[i][j])\n",
    "\n",
    "    # Sort subspecies by overlap intensity\n",
    "    subspecies_sorted = sorted(zip(subspecies, overlap_intensity), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Assign hues sequentially to prioritize highly overlapping subspecies\n",
    "    assigned_hues = {}\n",
    "    used_hues = set()\n",
    "    for subsp, _ in subspecies_sorted:\n",
    "        # Find the most distinct unused hue\n",
    "        best_hue = None\n",
    "        max_dist = -1\n",
    "        for i, hue in enumerate(hues):\n",
    "            if i in used_hues:\n",
    "                continue\n",
    "            # Check perceptual distance to already assigned hues\n",
    "            if assigned_hues:\n",
    "                dist = np.min(\n",
    "                    [min(abs(hue - assigned_hues[sp]), 360 - abs(hue - assigned_hues[sp])) for sp in assigned_hues]\n",
    "                )\n",
    "            else:\n",
    "                dist = float(\"inf\")\n",
    "            \n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                best_hue = i\n",
    "\n",
    "        # Assign the best hue\n",
    "        assigned_hues[subsp] = hues[best_hue]\n",
    "        used_hues.add(best_hue)\n",
    "    \n",
    "    return assigned_hues\n",
    "\n",
    "\n",
    "def get_bounds(geojson_result):\n",
    "    \"\"\"\n",
    "    Calculate the bounding box of all features in the GeoJSON.\n",
    "\n",
    "    Args:\n",
    "    - geojson_result: GeoJSON string with features.\n",
    "\n",
    "    Returns:\n",
    "    - Bounds as [[southwest_lat, southwest_lon], [northeast_lat, northeast_lon]].\n",
    "    \"\"\"\n",
    "    import json\n",
    "    #geojson_data = json.loads(geojson_result)\n",
    "    geojson_data = geojson_result\n",
    "    all_coords = []\n",
    "\n",
    "    for feature in geojson_data['features']:\n",
    "        # Extract all coordinates from the polygon or multipolygon\n",
    "        coords = feature['geometry']['coordinates']\n",
    "        if feature['geometry']['type'] == \"Polygon\":\n",
    "            all_coords.extend(coords[0])  # Add outer ring of the polygon\n",
    "        elif feature['geometry']['type'] == \"MultiPolygon\":\n",
    "            for poly in coords:\n",
    "                all_coords.extend(poly[0])  # Add outer ring of each polygon\n",
    "\n",
    "    # Extract longitudes (x) and latitudes (y) correctly\n",
    "    lons, lats = zip(*all_coords)\n",
    "    return [[min(lats), min(lons)], [max(lats), max(lons)]]\n",
    "\n",
    "\n",
    "def split_polygon_at_line(polygon, line):\n",
    "    \"\"\"\n",
    "    Splits a GeoJSON polygon at a given GeoJSON line.\n",
    "    \n",
    "    Parameters:\n",
    "        polygon (dict): shapely representation of the polygon.\n",
    "        line (dict): shapely representation of the line.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of GeoJSON polygons after splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform the split\n",
    "    split_result = split(polygon, line)\n",
    "    \n",
    "    # Convert Shapely polygons back to GeoJSON\n",
    "    split_geojsons = [mapping(geom) for geom in split_result.geoms]\n",
    "    \n",
    "    return split_geojsons\n",
    "\n",
    "\n",
    "def split_at_dateline(geojson):\n",
    "    \"\"\"\n",
    "    Splits polygons that cross the International Date Line (180Â° longitude).\n",
    "    \n",
    "    Parameters:\n",
    "    - geojson: A GeoJSON-like dictionary of geometries.\n",
    "    \n",
    "    Returns:\n",
    "    - A new GeoJSON with adjusted geometries.\n",
    "    \"\"\"\n",
    "    dateline = LineString([(180, 90), (180, -90)])\n",
    "    adjusted_polygons = []\n",
    "    \n",
    "    for feature in geojson['features']:\n",
    "        geom = feature['geometry']\n",
    "        polygon = Polygon(geom['coordinates'][0])\n",
    "\n",
    "        # Check if the polygon crosses the dateline\n",
    "        if not polygon.is_valid:\n",
    "            polygon = polygon.buffer(0)  # Fix invalid geometries\n",
    "\n",
    "        # Check if any of the polygon's coordinates are within 5 degrees of the dateline\n",
    "        if any(abs(lon) > 170 for lon, _ in polygon.exterior.coords) and any(abs(lon1 - lon2) > 180 for lon1, lon2 in zip(polygon.exterior.xy[0][:-1], polygon.exterior.xy[0][1:])):\n",
    "            # For all the longitudes that are less than -170, add 360 to them\n",
    "            new_coords = [\n",
    "                [(x + 360 if x < -170 else x, y) for x, y in polygon.exterior.coords]\n",
    "            ]\n",
    "            adjusted_polygons.append({\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": new_coords\n",
    "                },\n",
    "                \"properties\": feature['properties']\n",
    "            })\n",
    "        else:\n",
    "            adjusted_polygons.append(feature)\n",
    "    \n",
    "    return {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": adjusted_polygons\n",
    "    }\n",
    "\n",
    "\n",
    "def get_range_size(sp_cell_df, ssp):\n",
    "    \"\"\"Calculate the range size of a subspecies.\"\"\"\n",
    "    return (sp_cell_df[ssp] > 0).astype(int).sum()\n",
    "\n",
    "def get_color_mapping(sp_cell_df):\n",
    "    # Get the relationships between the infraspecies (ISSFs, forms, intergrades)\n",
    "    issfs, forms, intergrades, inter_to_p, form_to_p, top_inter, top_form = get_infraspecies_relationships(species, spp_dict)\n",
    "\n",
    "    # Get colors for the top-level infraspecies based on their geographic overlap\n",
    "    top_level_infras = [*issfs, *top_inter, *top_form]\n",
    "    sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "    overlap_matrix, top_level_infras = create_distribution_adjacency_matrix(sp_cell_df, top_level_infras)\n",
    "    ssp_hues = generate_priority_hues(top_level_infras, overlap_matrix)\n",
    "\n",
    "    # Get colors for the lower-level infraspecies by averaging their \"parent\" infraspecies\n",
    "    for intergrade, parents in inter_to_p.items():\n",
    "        ssp_hues[intergrade] = average_hues([ssp_hues[parent] for parent in parents])\n",
    "    for form, parents in form_to_p.items():\n",
    "        ssp_hues[form] = average_hues([ssp_hues[parent] for parent in parents])\n",
    "\n",
    "    # Convert hues to vibrant colors\n",
    "    ssp_colors = {ssp: hue_to_hex_vibrant(hue) for ssp, hue in ssp_hues.items()}\n",
    "\n",
    "    # Organize subspecies into categories and calculate their range sizes (from largest to smallest)\n",
    "    sorted_issfs = sorted(issfs, key=lambda ssp: get_range_size(sp_cell_df, ssp), reverse=True)\n",
    "    sorted_forms = sorted(forms, key=lambda ssp: get_range_size(sp_cell_df, ssp), reverse=True)\n",
    "    sorted_intergrades = sorted(intergrades, key=lambda ssp: get_range_size(sp_cell_df, ssp), reverse=True)\n",
    "\n",
    "    return ssp_colors, sorted_issfs, sorted_forms, sorted_intergrades\n",
    "\n",
    "def get_ssp_common_name(species, subsp):\n",
    "    # Look up the PRIMARY_COM_NAME for the subspecies\n",
    "    subsp_name = taxonomy[taxonomy['SCI_NAME'] == f\"{species} {subsp}\"].PRIMARY_COM_NAME.values[0]\n",
    "    # Get the part in the parentheses\n",
    "    subsp_name = subsp_name.split('(')[-1].split(')')[0] if '(' in subsp_name else subsp\n",
    "    return subsp_name\n",
    "\n",
    "\n",
    "def reduce_precision(geojson_data, precision):\n",
    "    def round_coords(coords):\n",
    "        return [[round(coord[0], precision), round(coord[1], precision)] for coord in coords]\n",
    "    \n",
    "    def process_feature(feature):\n",
    "        if feature['geometry']['type'] == 'Polygon':\n",
    "            feature['geometry']['coordinates'] = [round_coords(coords) for coords in feature['geometry']['coordinates']]\n",
    "        elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "            feature['geometry']['coordinates'] = [\n",
    "                [round_coords(coords) for coords in polygon]\n",
    "                for polygon in feature['geometry']['coordinates']\n",
    "            ]\n",
    "        return feature\n",
    "\n",
    "    geojson_data['features'] = [process_feature(feature) for feature in geojson_data['features']]\n",
    "    return geojson_data\n",
    "\n",
    "def choropleth_map(sp_cell_df, common_name, subspp_colors, sorted_issfs, sorted_forms, sorted_intergrades):\n",
    "    \"\"\"Creates a choropleth map given species data.\"\"\"\n",
    "    \n",
    "    f = folium.Figure()\n",
    "    map = folium.Map(location=[47, -122], zoom_start=5, tiles=\"cartodbpositron\", control_scale=True)\n",
    "    f.add_child(map)\n",
    "\n",
    "    sp = sp_cell_df.columns[0]\n",
    "    subspp = sp_cell_df.columns[1:]\n",
    "    \n",
    "    # Create polygon features with \n",
    "    list_features = []\n",
    "    for _, row in sp_cell_df.iterrows():\n",
    "        # Calculate the relative percentage of sightings of each subspecies for each cell\n",
    "        percentages = (row[subspp] / sum(row[subspp])) * 100\n",
    "        percentages = round(percentages, 0)\n",
    "        percentages_dict = percentages.to_dict()\n",
    "        \n",
    "        # Calculate total reports across all subspecies for each cell\n",
    "        total_reports = row[subspp].sum()\n",
    "\n",
    "        # Precompute tooltip text showing only subspecies with nonzero percentages\n",
    "        percentages_dict_ordered = pd.DataFrame(percentages_dict, index=['pct']).T.query('pct > 0')['pct'].sort_values(ascending=False).to_dict()\n",
    "        \n",
    "        # Start tooltip container\n",
    "        tt1_style = '''.tt1 {min-width: 100px; max-width: 300px; overflow: auto;}'''\n",
    "        tooltip_text = '<div class=\"tt1\">'\n",
    "\n",
    "        # Add header and create a flex container for total reports and percentages\n",
    "\n",
    "        tt2_style = '''.tt2 {display: flex; justify-content: space-between; margin-top: 5px;}'''\n",
    "        tt3_style = '''.tt3 {margin-right: 10px;}'''\n",
    "        tt4_style = '''.tt4 {text-align: right; background: ;}'''\n",
    "        total_reports = row[subspp].sum()\n",
    "        tooltip_text += f'''<div class=\"tt2\"><div class=\"tt3\"><strong>Reported taxa</strong><br>Total reports: {total_reports:.0f}</div><div class=\"tt4\">'''\n",
    "        # Add percentages to the right-aligned div\n",
    "        if total_reports > 0:\n",
    "            for subsp, percent in percentages_dict_ordered.items():\n",
    "                if percent > 0:\n",
    "                    subsp_common_name = get_ssp_common_name(sp, subsp)\n",
    "                    tooltip_text += f\"<div>{subsp_common_name}: {percent:.0f}%</div>\" if percent > 0.5 else \"\"\n",
    "        else:\n",
    "            tooltip_text += '''</div>None</div>'''\n",
    "\n",
    "        # Close all divs\n",
    "        tooltip_text += '''</div></div></div>'''\n",
    "\n",
    "        # Add the tooltip to the feature's properties\n",
    "        percentages_dict_ordered[\"tooltip\"] = tooltip_text\n",
    "\n",
    "        # Convert the H3 cell into a geometry\n",
    "        geometry_for_row = h3.cells_to_geo(cells=[row.name])\n",
    "\n",
    "        # Add a GeoJSON Feature to the list of features\n",
    "        feature = Feature(\n",
    "            geometry=geometry_for_row,\n",
    "            id=row.name,\n",
    "            properties=percentages_dict_ordered\n",
    "        )\n",
    "        list_features.append(feature)\n",
    "\n",
    "    # Create a GeoJSON FeatureCollection with the list of features\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "\n",
    "    # Deal with geometries that cross the International Date Line\n",
    "    geojson_result = split_at_dateline(json.loads(geojson_result))\n",
    "\n",
    "    # Reduce precision to 3 digits (111 meters)\n",
    "    geojson_result = reduce_precision(geojson_result, precision=3)\n",
    "    \n",
    "    # Add GeoJSON layer to the map\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: style_function(feature, subspp_colors),\n",
    "        name=f'{sp} Subspecies Map'\n",
    "    ).add_to(map)\n",
    "    \n",
    "    # Add a tooltip to the GeoJSON layer\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: {\n",
    "            'weight': 0, \n",
    "            'color': 'transparent',  \n",
    "            'fillOpacity': 0.6 \n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            fields=[\"tooltip\"],\n",
    "            aliases=[None],  # No additional label prepended\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            labels=False,  # Disable default labels\n",
    "            html=True  # Enable HTML formatting in the tooltip\n",
    "        )\n",
    "    ).add_to(map)\n",
    "\n",
    "\n",
    "    # Add legend with subheaders\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"position: fixed; top: 10px; right: 10px; max-width: 200px; height: auto; z-index: 9999; background-color: white; box-shadow: 0 0 5px rgba(0, 0, 0, 0.2); border: 1px solid lightgray; border-radius: 5px; padding: 10px; font-size: 10px;\">\n",
    "        <div style=\"font-size: 12px;\"><strong>{common_name}</strong></div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add subheaders and sorted species\n",
    "    issf_colors = {k: subspp_colors[k] for k in sorted_issfs}\n",
    "    form_colors = {k: subspp_colors[k] for k in sorted_forms}\n",
    "    intergrade_colors = {k: subspp_colors[k] for k in sorted_intergrades}\n",
    "    categories = [(\"Subspecies\", issf_colors), (\"Forms\", form_colors), (\"Intergrades\", intergrade_colors)]\n",
    "    # Remove any categories where the dictionary has no keys\n",
    "    categories = [(category, ssp_in_category) for category, ssp_in_category in categories if ssp_in_category]\n",
    "\n",
    "    # Create species and colors part of legend\n",
    "    for category, sorted_subspecies in categories:\n",
    "        legend_html += f'<div style=\"margin-top:10px;\"><b>{category}</b></div>'\n",
    "        for subsp in sorted_subspecies:\n",
    "            subsp_common_name = get_ssp_common_name(sp, subsp)\n",
    "            if subsp_common_name == subsp.strip('[]'): # Only display one name if no unique common name\n",
    "                display_name = subsp\n",
    "            else:\n",
    "                display_name = f\"{subsp_common_name} ({subsp})\"\n",
    "\n",
    "            display_name = '/<wbr>'.join(display_name.split('/')) # Add available wordbreaks for long slash names\n",
    "            color = subspp_colors.get(subsp, \"#ccc\")  # Default color if no color is found\n",
    "\n",
    "            # Get link to the subspecies's eBird species account\n",
    "            subsp_code = taxonomy[taxonomy['SCI_NAME'] == f\"{sp} {subsp}\"].SPECIES_CODE.values[0]\n",
    "            subsp_link = f\"https://ebird.org/species/{subsp_code}\"\n",
    "            \n",
    "\n",
    "            subsp_display = f\"\"\"\n",
    "            <div style=\"display: inline-block; max-width: 150px; white-space: normal; overflow-wrap: break-word;\">\n",
    "                <a href=\"{subsp_link}\" target=\"_blank\">{display_name}</a>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            # Format subspecies name next to colors\n",
    "            legend_html += f\"\"\"\n",
    "            <div style=\"margin-top: 5px;\">\n",
    "                <span style=\"display: inline-block; width: 20px; height: 10px; margin-right: 5px; background-color: {color};\"></span>\n",
    "                {subsp_display}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "    legend_html += \"</div>\"\n",
    "    \n",
    "    legend_element = folium.Element(legend_html)\n",
    "    map.get_root().html.add_child(legend_element)\n",
    "    map.get_root().html.add_child(folium.Element(f\"\"\"\n",
    "        <style>\n",
    "            {tt1_style}\n",
    "            {tt2_style}\n",
    "            {tt3_style}\n",
    "            {tt4_style}\n",
    "        </style>\"\"\"))\n",
    "\n",
    "    # Calculate bounds and adjust the map's view\n",
    "    bounds = get_bounds(geojson_result)\n",
    "    map.fit_bounds(bounds)\n",
    "\n",
    "    # TODO: String manipulations to make HTML smaller?\n",
    "    string_so_far = map.get_root().render()\n",
    "    return string_so_far\n",
    "\n",
    "\n",
    "remake_maps = True\n",
    "for sp_code in sp_codes:\n",
    "    subspp_colors = None\n",
    "    print(\"\\nMapping\", sp_code)\n",
    "    common_name = taxonomy[taxonomy['SPECIES_CODE'] == sp_code].PRIMARY_COM_NAME.values[0]\n",
    "    for resolution in resolutions: #[2,3,4]\n",
    "        species = taxonomy[taxonomy['PRIMARY_COM_NAME'] == common_name].SCI_NAME.values[0]\n",
    "        dataname = f\"sp_cell_dfs/{species.replace(' ', '-')}_resolution{resolution}.csv\"\n",
    "        if not Path(dataname).exists():\n",
    "            print(\"No data for\", species, \"at resolution\", resolution)\n",
    "            continue\n",
    "        map_filename = f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\"\n",
    "        if Path(map_filename).exists() and not remake_maps:\n",
    "            print(\"Map already exists for\", species, \"at resolution\", resolution)\n",
    "            continue\n",
    "        sp_cell_df = pd.read_csv(dataname, index_col=0)\n",
    "        sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "        subspecies = sp_cell_df.columns[1:]\n",
    "        if resolution == 2 or subspp_colors == None:\n",
    "            subspp_colors, sorted_issfs, sorted_forms, sorted_intergrades = get_color_mapping(sp_cell_df)\n",
    "            \n",
    "        m = choropleth_map(sp_cell_df, common_name, subspp_colors, sorted_issfs, sorted_forms, sorted_intergrades)\n",
    "        map_filename = f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\"\n",
    "        #m.save(map_filename)\n",
    "        with open(map_filename, 'w') as f:\n",
    "            f.write(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CSV of map URLs for the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subspeciesmapper.netlify.app/Branta-bernicla_3.html\n",
      "https://subspeciesmapper.netlify.app/Branta-bernicla_2.html\n",
      "https://subspeciesmapper.netlify.app/Branta-bernicla_4.html\n",
      "https://subspeciesmapper.netlify.app/Branta-hutchinsii_4.html\n",
      "https://subspeciesmapper.netlify.app/Branta-hutchinsii_2.html\n",
      "https://subspeciesmapper.netlify.app/Branta-hutchinsii_3.html\n",
      "https://subspeciesmapper.netlify.app/Branta-canadensis_4.html\n",
      "https://subspeciesmapper.netlify.app/Branta-canadensis_3.html\n",
      "https://subspeciesmapper.netlify.app/Branta-canadensis_2.html\n",
      "https://subspeciesmapper.netlify.app/Somateria-mollissima_4.html\n",
      "https://subspeciesmapper.netlify.app/Somateria-mollissima_2.html\n",
      "https://subspeciesmapper.netlify.app/Somateria-mollissima_3.html\n",
      "https://subspeciesmapper.netlify.app/Butorides-striata_4.html\n",
      "https://subspeciesmapper.netlify.app/Butorides-striata_2.html\n",
      "https://subspeciesmapper.netlify.app/Butorides-striata_3.html\n",
      "https://subspeciesmapper.netlify.app/Buteo-jamaicensis_4.html\n",
      "https://subspeciesmapper.netlify.app/Buteo-jamaicensis_2.html\n",
      "https://subspeciesmapper.netlify.app/Buteo-jamaicensis_3.html\n",
      "https://subspeciesmapper.netlify.app/Colaptes-auratus_4.html\n",
      "https://subspeciesmapper.netlify.app/Colaptes-auratus_3.html\n",
      "https://subspeciesmapper.netlify.app/Colaptes-auratus_2.html\n",
      "https://subspeciesmapper.netlify.app/Falco-peregrinus_4.html\n",
      "https://subspeciesmapper.netlify.app/Falco-peregrinus_2.html\n",
      "https://subspeciesmapper.netlify.app/Falco-peregrinus_3.html\n",
      "https://subspeciesmapper.netlify.app/Garrulus-glandarius_4.html\n",
      "https://subspeciesmapper.netlify.app/Garrulus-glandarius_2.html\n",
      "https://subspeciesmapper.netlify.app/Garrulus-glandarius_3.html\n",
      "https://subspeciesmapper.netlify.app/Periparus-ater_4.html\n",
      "https://subspeciesmapper.netlify.app/Periparus-ater_2.html\n",
      "https://subspeciesmapper.netlify.app/Periparus-ater_3.html\n",
      "https://subspeciesmapper.netlify.app/Eremophila-alpestris_2.html\n",
      "https://subspeciesmapper.netlify.app/Eremophila-alpestris_3.html\n",
      "https://subspeciesmapper.netlify.app/Eremophila-alpestris_4.html\n",
      "https://subspeciesmapper.netlify.app/Motacilla-alba_4.html\n",
      "https://subspeciesmapper.netlify.app/Motacilla-alba_3.html\n",
      "https://subspeciesmapper.netlify.app/Motacilla-alba_2.html\n",
      "https://subspeciesmapper.netlify.app/Loxia-curvirostra_3.html\n",
      "https://subspeciesmapper.netlify.app/Loxia-curvirostra_2.html\n",
      "https://subspeciesmapper.netlify.app/Loxia-curvirostra_4.html\n",
      "https://subspeciesmapper.netlify.app/Passerella-iliaca_3.html\n",
      "https://subspeciesmapper.netlify.app/Passerella-iliaca_2.html\n",
      "https://subspeciesmapper.netlify.app/Passerella-iliaca_4.html\n",
      "https://subspeciesmapper.netlify.app/Junco-hyemalis_4.html\n",
      "https://subspeciesmapper.netlify.app/Junco-hyemalis_2.html\n",
      "https://subspeciesmapper.netlify.app/Junco-hyemalis_3.html\n",
      "https://subspeciesmapper.netlify.app/Zonotrichia-leucophrys_3.html\n",
      "https://subspeciesmapper.netlify.app/Zonotrichia-leucophrys_2.html\n",
      "https://subspeciesmapper.netlify.app/Zonotrichia-leucophrys_4.html\n",
      "https://subspeciesmapper.netlify.app/Icteria-virens_2.html\n",
      "https://subspeciesmapper.netlify.app/Icteria-virens_3.html\n",
      "https://subspeciesmapper.netlify.app/Icteria-virens_4.html\n",
      "https://subspeciesmapper.netlify.app/Sturnella-magna_4.html\n",
      "https://subspeciesmapper.netlify.app/Sturnella-magna_3.html\n",
      "https://subspeciesmapper.netlify.app/Sturnella-magna_2.html\n",
      "https://subspeciesmapper.netlify.app/Leiothlypis-celata_4.html\n",
      "https://subspeciesmapper.netlify.app/Leiothlypis-celata_2.html\n",
      "https://subspeciesmapper.netlify.app/Leiothlypis-celata_3.html\n",
      "https://subspeciesmapper.netlify.app/Setophaga-coronata_3.html\n",
      "https://subspeciesmapper.netlify.app/Setophaga-coronata_2.html\n",
      "https://subspeciesmapper.netlify.app/Setophaga-coronata_4.html\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"common_name\", \"scientific_name\", \"resolution\", \"map_url\"])\n",
    "maps_dir = Path(\"docs/maps\")\n",
    "\n",
    "# Sort maps taxonomically\n",
    "maps_list = list(maps_dir.glob(\"*.html\"))\n",
    "species = {p.name.split('_')[0].replace('-', ' '): p for p in maps_list}\n",
    "ordering = taxonomy[['SCI_NAME', 'TAXON_ORDER']].set_index(\"SCI_NAME\").to_dict()['TAXON_ORDER']\n",
    "maps_list = sorted(maps_list, key=lambda x: ordering[x.name.split('_')[0].replace('-', ' ')])\n",
    "\n",
    "# Create table of common name, species, resolution, and map URL\n",
    "# This is used by the website\n",
    "for idx, file in enumerate(maps_list):\n",
    "    resolution = file.stem.split(\"_\")[-1]\n",
    "    species = file.stem.replace(f\"_{resolution}\", \"\")\n",
    "    common_name = taxonomy[taxonomy['SCI_NAME'] == species.replace('-', ' ')].PRIMARY_COM_NAME.values[0]\n",
    "    #map_url = Path(Path(file).parent.stem).joinpath(Path(file).name)\n",
    "    map_url = 'https://subspeciesmapper.netlify.app/' + str(file.relative_to(maps_dir))\n",
    "    print(map_url)\n",
    "    df.loc[idx] = [common_name, species, resolution, map_url]\n",
    "df.to_csv(\"docs/data/map_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
