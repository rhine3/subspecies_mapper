{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import h3\n",
    "import folium\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "pd.options.mode.copy_on_write = True \n",
    "from functools import reduce\n",
    "from folium import GeoJsonTooltip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON of subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAXON_ORDER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>TAXON_CONCEPT_ID</th>\n",
       "      <th>PRIMARY_COM_NAME</th>\n",
       "      <th>SCI_NAME</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>SPECIES_GROUP</th>\n",
       "      <th>REPORT_AS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common Ostrich</td>\n",
       "      <td>Struthio camelus</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Somali Ostrich</td>\n",
       "      <td>Struthio molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>slash</td>\n",
       "      <td>y00934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common/Somali Ostrich</td>\n",
       "      <td>Struthio camelus/molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>species</td>\n",
       "      <td>soucas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southern Cassowary</td>\n",
       "      <td>Casuarius casuarius</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>species</td>\n",
       "      <td>dwacas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dwarf Cassowary</td>\n",
       "      <td>Casuarius bennetti</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TAXON_ORDER CATEGORY SPECIES_CODE  TAXON_CONCEPT_ID       PRIMARY_COM_NAME  \\\n",
       "0            2  species      ostric2               NaN         Common Ostrich   \n",
       "1            7  species      ostric3               NaN         Somali Ostrich   \n",
       "2            8    slash       y00934               NaN  Common/Somali Ostrich   \n",
       "3           10  species      soucas1               NaN     Southern Cassowary   \n",
       "4           11  species      dwacas1               NaN        Dwarf Cassowary   \n",
       "\n",
       "                         SCI_NAME             ORDER  \\\n",
       "0                Struthio camelus  Struthioniformes   \n",
       "1          Struthio molybdophanes  Struthioniformes   \n",
       "2  Struthio camelus/molybdophanes  Struthioniformes   \n",
       "3             Casuarius casuarius    Casuariiformes   \n",
       "4              Casuarius bennetti    Casuariiformes   \n",
       "\n",
       "                              FAMILY        SPECIES_GROUP REPORT_AS  \n",
       "0          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "1          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "2          Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "3  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "4  Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  "
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load eBird taxonomy\n",
    "taxonomy = pd.read_csv(\"/Users/tessa/Code/scratchpad/subspecies_plotter/eBird_taxonomy_v2024.csv\")\n",
    "taxonomy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['species', 'slash', 'issf', 'hybrid', 'spuh', 'domestic', 'form',\n",
       "       'intergrade'], dtype=object)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What categories are there in the taxonomy?\n",
    "taxonomy.CATEGORY.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11145"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many species are there?\n",
    "species = taxonomy[taxonomy.CATEGORY == 'species']\n",
    "len(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3843"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many infraspecific entries are there?\n",
    "infrasp_categories = ['issf', 'form', 'intergrade']\n",
    "infraspp = taxonomy[taxonomy.CATEGORY.isin(infrasp_categories)]\n",
    "len(infraspp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdated: Clements taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Clements taxonomy\n",
    "# taxonomy = pd.read_csv(\"/Users/tessa/Code/scratchpad/subspecies_plotter/Clements-v2024-October-2024-rev.csv\")\n",
    "# taxonomy.head()\n",
    "\n",
    "# What categories are there in the taxonomy?\n",
    "# taxonomy.category.unique()\n",
    "\n",
    "# How many species are there?\n",
    "# species = taxonomy[taxonomy.category == 'species']\n",
    "# len(species)\n",
    "\n",
    "# How many infraspecific entries are there?\n",
    "#infrasp_categories = ['subspecies', 'group (monotypic)', 'group (polytypic)', 'form']\n",
    "#infraspp = taxonomy[taxonomy.category.isin(infrasp_categories)]\n",
    "#len(infraspp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAXON_ORDER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>TAXON_CONCEPT_ID</th>\n",
       "      <th>PRIMARY_COM_NAME</th>\n",
       "      <th>SCI_NAME</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>FAMILY</th>\n",
       "      <th>SPECIES_GROUP</th>\n",
       "      <th>REPORT_AS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common Ostrich</td>\n",
       "      <td>Struthio camelus</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>species</td>\n",
       "      <td>ostric3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Somali Ostrich</td>\n",
       "      <td>Struthio molybdophanes</td>\n",
       "      <td>Struthioniformes</td>\n",
       "      <td>Struthionidae (Ostriches)</td>\n",
       "      <td>Ostriches</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>species</td>\n",
       "      <td>soucas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southern Cassowary</td>\n",
       "      <td>Casuarius casuarius</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>species</td>\n",
       "      <td>dwacas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dwarf Cassowary</td>\n",
       "      <td>Casuarius bennetti</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>species</td>\n",
       "      <td>norcas1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northern Cassowary</td>\n",
       "      <td>Casuarius unappendiculatus</td>\n",
       "      <td>Casuariiformes</td>\n",
       "      <td>Casuariidae (Cassowaries and Emu)</td>\n",
       "      <td>Cassowaries and Emu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17406</th>\n",
       "      <td>35548</td>\n",
       "      <td>species</td>\n",
       "      <td>thbsal1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thick-billed Saltator</td>\n",
       "      <td>Saltator maxillosus</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Thraupidae (Tanagers and Allies)</td>\n",
       "      <td>Tanagers and Allies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17407</th>\n",
       "      <td>35549</td>\n",
       "      <td>species</td>\n",
       "      <td>gobsal1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Golden-billed Saltator</td>\n",
       "      <td>Saltator aurantiirostris</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Thraupidae (Tanagers and Allies)</td>\n",
       "      <td>Tanagers and Allies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17408</th>\n",
       "      <td>35556</td>\n",
       "      <td>species</td>\n",
       "      <td>massal1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Masked Saltator</td>\n",
       "      <td>Saltator cinctus</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Thraupidae (Tanagers and Allies)</td>\n",
       "      <td>Tanagers and Allies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17409</th>\n",
       "      <td>35557</td>\n",
       "      <td>species</td>\n",
       "      <td>slcgro1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slate-colored Grosbeak</td>\n",
       "      <td>Saltator grossus</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Thraupidae (Tanagers and Allies)</td>\n",
       "      <td>Tanagers and Allies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17410</th>\n",
       "      <td>35560</td>\n",
       "      <td>species</td>\n",
       "      <td>bltgro2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Black-throated Grosbeak</td>\n",
       "      <td>Saltator fuliginosus</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Thraupidae (Tanagers and Allies)</td>\n",
       "      <td>Tanagers and Allies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11145 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TAXON_ORDER CATEGORY SPECIES_CODE  TAXON_CONCEPT_ID  \\\n",
       "0                2  species      ostric2               NaN   \n",
       "1                7  species      ostric3               NaN   \n",
       "3               10  species      soucas1               NaN   \n",
       "4               11  species      dwacas1               NaN   \n",
       "5               12  species      norcas1               NaN   \n",
       "...            ...      ...          ...               ...   \n",
       "17406        35548  species      thbsal1               NaN   \n",
       "17407        35549  species      gobsal1               NaN   \n",
       "17408        35556  species      massal1               NaN   \n",
       "17409        35557  species      slcgro1               NaN   \n",
       "17410        35560  species      bltgro2               NaN   \n",
       "\n",
       "              PRIMARY_COM_NAME                    SCI_NAME             ORDER  \\\n",
       "0               Common Ostrich            Struthio camelus  Struthioniformes   \n",
       "1               Somali Ostrich      Struthio molybdophanes  Struthioniformes   \n",
       "3           Southern Cassowary         Casuarius casuarius    Casuariiformes   \n",
       "4              Dwarf Cassowary          Casuarius bennetti    Casuariiformes   \n",
       "5           Northern Cassowary  Casuarius unappendiculatus    Casuariiformes   \n",
       "...                        ...                         ...               ...   \n",
       "17406    Thick-billed Saltator         Saltator maxillosus     Passeriformes   \n",
       "17407   Golden-billed Saltator    Saltator aurantiirostris     Passeriformes   \n",
       "17408          Masked Saltator            Saltator cinctus     Passeriformes   \n",
       "17409   Slate-colored Grosbeak            Saltator grossus     Passeriformes   \n",
       "17410  Black-throated Grosbeak        Saltator fuliginosus     Passeriformes   \n",
       "\n",
       "                                  FAMILY        SPECIES_GROUP REPORT_AS  \n",
       "0              Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "1              Struthionidae (Ostriches)            Ostriches       NaN  \n",
       "3      Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "4      Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "5      Casuariidae (Cassowaries and Emu)  Cassowaries and Emu       NaN  \n",
       "...                                  ...                  ...       ...  \n",
       "17406   Thraupidae (Tanagers and Allies)  Tanagers and Allies       NaN  \n",
       "17407   Thraupidae (Tanagers and Allies)  Tanagers and Allies       NaN  \n",
       "17408   Thraupidae (Tanagers and Allies)  Tanagers and Allies       NaN  \n",
       "17409   Thraupidae (Tanagers and Allies)  Tanagers and Allies       NaN  \n",
       "17410   Thraupidae (Tanagers and Allies)  Tanagers and Allies       NaN  \n",
       "\n",
       "[11145 rows x 10 columns]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d553cadcb99344c5ae9f5416524b6c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/365534950.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mspp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPECIES_CODE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PRIMARY_COM_NAME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SCI_NAME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SCI_NAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Add infraspecies to spp_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspp_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Get infraspecies for this species\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0minfraspp_for_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfraspp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfraspp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SCI_NAME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0minfraspp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Add infraspecies to spp_json by category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6199\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[0;32m-> 6203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3856\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3857\u001b[0m         \u001b[0mcheck_dict_or_set_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3858\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3859\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping species to their infraspecies, by category\n",
    "# Most species have a single category of infraspecies (e.g. either form or subspecies, not both)\n",
    "# However some species have infraspecies in multiple categories, e.g. Brant (Branta bernicla) has both subspecies and forms\n",
    "spp_dict = species[['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "# Add infraspecies to spp_json\n",
    "for sp in tqdm(spp_dict.keys()):\n",
    "    # Get infraspecies for this species\n",
    "    infraspp_for_sp = infraspp[infraspp['SCI_NAME'].apply(lambda x: x[:len(sp)] == sp)]\n",
    "    infraspp_dict = dict()\n",
    "\n",
    "    # Add infraspecies to spp_json by category\n",
    "    for cat in infrasp_categories:\n",
    "        infrasp_in_category = infraspp_for_sp[infraspp_for_sp.CATEGORY == cat]\n",
    "        infrasp_cat_dict = infrasp_in_category[\n",
    "            ['SPECIES_CODE', 'PRIMARY_COM_NAME', 'SCI_NAME']].set_index(\"SCI_NAME\").T.to_dict()\n",
    "        if len(infrasp_cat_dict.keys()) > 0:\n",
    "            infraspp_dict[cat] = infrasp_cat_dict\n",
    "    spp_dict[sp]['infraspecies'] = infraspp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/tessa/Code/scratchpad/subspecies_plotter/infraspecies_ebird.json\", 'w') as f:\n",
    "    f.write(json.dumps(spp_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep eBird data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H3 resolutions\n",
    "```\n",
    "Res\tAverage Hexagon Area (km2)\tPentagon Area* (km2)\tRatio (P/H)\n",
    "0\t4,357,449.416078381\t2,562,182.162955496\t0.5880\n",
    "1\t609,788.441794133\t328,434.586246469\t0.5386\n",
    "2\t86,801.780398997\t44,930.898497879\t0.5176\n",
    "3\t12,393.434655088\t6,315.472267516\t0.5096\n",
    "4\t1,770.347654491\t896.582383141\t0.5064\n",
    "5\t252.903858182\t127.785583023\t0.5053\n",
    "6\t36.129062164\t18.238749548\t0.5048\n",
    "7\t5.161293360\t2.604669397\t0.5047\n",
    "```\n",
    "\n",
    "H3 number of cells\n",
    "```\n",
    "Res\tTotal number of cells\tNumber of hexagons\tNumber of pentagons\n",
    "0\t122\t110\t12\n",
    "1\t842\t830\t12\n",
    "2\t5,882\t5,870\t12\n",
    "3\t41,162\t41,150\t12\n",
    "4\t288,122\t288,110\t12\n",
    "5\t2,016,842\t2,016,830\t12\n",
    "6\t14,117,882\t14,117,870\t12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine number of sightings of each subspecies per grid cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing strher\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing easmea\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing yerwar\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing eurjay1\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing brant\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing whcspa\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing cacgoo1\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing horlar\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing coatit2\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing foxspa\n",
      "No more data to process\n",
      "\n",
      "\n",
      "\n",
      "Processing daejun\n",
      "Last chunk, total rows in dataset: 14296832\n",
      "\n",
      "\n",
      "\n",
      "Processing orcwar\n",
      "Last chunk, total rows in dataset: 2834032\n",
      "\n",
      "\n",
      "\n",
      "Processing yebcha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/2730103252.py:130: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 944389\n",
      "\n",
      "\n",
      "\n",
      "Processing cangoo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/2730103252.py:130: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 20222796\n",
      "\n",
      "\n",
      "\n",
      "Processing whiwag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/2730103252.py:130: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 2793921\n",
      "\n",
      "\n",
      "\n",
      "Processing norfli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/2730103252.py:130: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last chunk, total rows in dataset: 14604156\n",
      "\n",
      "\n",
      "\n",
      "Processing rethaw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/c2h76xs16cn4jt3vqc68b5lm0000gr/T/ipykernel_34106/2730103252.py:130: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[566], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Get data on presence of each subspp for each resolution\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resolution \u001b[38;5;129;01min\u001b[39;00m resolutions:\n\u001b[0;32m--> 159\u001b[0m     species_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_species_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaned_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubspp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     filename \u001b[38;5;241m=\u001b[39m ssp_batch_directory\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_row\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(start_idx\u001b[38;5;241m+\u001b[39midx)\u001b[38;5;241m*\u001b[39mchunk_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_row\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_resolution\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m     species_df\u001b[38;5;241m.\u001b[39mto_csv(filename)\n",
      "Cell \u001b[0;32mIn[566], line 72\u001b[0m, in \u001b[0;36mget_species_df\u001b[0;34m(sp, sp_df, subspp, resolution)\u001b[0m\n\u001b[1;32m     70\u001b[0m cell_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m sp_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhex_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m---> 72\u001b[0m     cell_df \u001b[38;5;241m=\u001b[39m sp_df[\u001b[43msp_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhex_id_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresolution\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m]\n\u001b[1;32m     73\u001b[0m     cell_data \u001b[38;5;241m=\u001b[39m get_grid_cell_species_data(cell_df, sp, subspp, resolution)\n\u001b[1;32m     74\u001b[0m     cell_dicts\u001b[38;5;241m.\u001b[39mappend(cell_data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5796\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5797\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5799\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/opso-dev/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "resolutions = [2,3,4,5]\n",
    "def clean_ebd(\n",
    "    full_df,\n",
    "    remove_unconfirmed=True, \n",
    "    remove_reviewed=False,\n",
    "    resolutions = resolutions,\n",
    "):\n",
    "\n",
    "    # Remove duplicate checklists\n",
    "    full_df = full_df[full_df['GROUP IDENTIFIER'].isnull() | ~full_df[full_df['GROUP IDENTIFIER'].notnull()].duplicated(subset=[\"GROUP IDENTIFIER\", \"SCIENTIFIC NAME\"],keep='first')]\n",
    "\n",
    "    # Removed unconfirmed observations or reviewed observations, if desired\n",
    "    if remove_unconfirmed:\n",
    "        full_df = full_df[full_df[\"APPROVED\"] == 1]\n",
    "    if remove_reviewed:\n",
    "        full_df = full_df[full_df[\"REVIEWED\"] == 0]\n",
    "\n",
    "    # Just subset to the needed columns\n",
    "    needed_columns = [\n",
    "        'TAXONOMIC ORDER','CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', \n",
    "        'SCIENTIFIC NAME','SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "        'SAMPLING EVENT IDENTIFIER',\n",
    "        'LATITUDE', 'LONGITUDE', 'REVIEWED', 'OBSERVATION DATE']\n",
    "    full_df = full_df[needed_columns]\n",
    "    full_df.head()\n",
    "\n",
    "    # Convert latitude and longitude to an H3 hexagon ID\n",
    "    for resolution in resolutions:\n",
    "        full_df[f'hex_id_{resolution}'] = full_df.apply(lambda row:  h3.latlng_to_cell(row.LATITUDE, row.LONGITUDE, resolution), axis=1)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "def get_grid_cell_species_data(cell_df, sp, subspp, resolution):\n",
    "    \"\"\"Get # of checklists containing a species and each subspecies\n",
    "\n",
    "    Args:\n",
    "    - cell_df: pd.DataFrame, dataframe of data for a single grid cell (1 row per observation)\n",
    "    - sp: str, scientific name of species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "\n",
    "    Returns:\n",
    "    - cell_data: dict, with keys 'cell_id', species name, and subspecies names\n",
    "    \"\"\"\n",
    "    # Total number of checklists containing the species\n",
    "    num_checklists = cell_df[\"SAMPLING EVENT IDENTIFIER\"].nunique()\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_data = {'cell_id': cell_df[f\"hex_id_{resolution}\"].iloc[0]}\n",
    "    cell_data[sp] = num_checklists\n",
    "\n",
    "    # Add number of checklists containing each subspecies\n",
    "    for subsp in subspp:\n",
    "        num_subsp = cell_df[cell_df[\"SUBSPECIES SCIENTIFIC NAME\"] == subsp].shape[0]\n",
    "        cell_data[subsp] = num_subsp\n",
    "\n",
    "    return cell_data\n",
    "\n",
    "\n",
    "def get_species_df(sp, sp_df, subspp, resolution):\n",
    "    \"\"\"Make dataframe of species & subspecies data for every cell for a given species\n",
    "\n",
    "    Args:\n",
    "    - sp: str, scientific name of species\n",
    "    - df: pd.DataFrame, dataframe of data for this species\n",
    "    - subspp: list of str, scientific names of subspecies for this species\n",
    "    - resolution: int, H3 resolution level\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dict of # checklists containing sp for all cells\n",
    "    cell_dicts = []\n",
    "    for cell in sp_df[f\"hex_id_{resolution}\"].unique():\n",
    "        cell_df = sp_df[sp_df[f\"hex_id_{resolution}\"] == cell]\n",
    "        cell_data = get_grid_cell_species_data(cell_df, sp, subspp, resolution)\n",
    "        cell_dicts.append(cell_data)\n",
    "\n",
    "    sp_cell_df = pd.DataFrame(cell_dicts, index=range(len(cell_dicts)))\n",
    "    sp_cell_df.set_index(\"cell_id\", inplace=True)\n",
    "\n",
    "    return sp_cell_df\n",
    "\n",
    "\n",
    "\n",
    "spp_dict = json.load(open(\"/Users/tessa/Code/scratchpad/subspecies_plotter/infraspecies_ebird.json\"))\n",
    "\n",
    "# caja_df = get_species_df(sp, full_df, subspp_dict)\n",
    "# filepath = Path('/Users/tessa/Code/scratchpad/subspecies_plotter/batches').joinpath(filename)\n",
    "# caja_df.to_csv(filepath)\n",
    "\n",
    "#dataset_filepath = \"/Users/tessa/Code/scratchpad/subspecies_plotter/ebd-sample.txt\"\n",
    "#sp_code = 'rethaw'\n",
    "use_cols = [\n",
    "        'TAXONOMIC ORDER','CATEGORY', 'TAXON CONCEPT ID', 'COMMON NAME', \n",
    "        'SCIENTIFIC NAME','SUBSPECIES COMMON NAME', 'SUBSPECIES SCIENTIFIC NAME',\n",
    "        'SAMPLING EVENT IDENTIFIER',\n",
    "        'LATITUDE', 'LONGITUDE', 'REVIEWED', 'APPROVED', 'GROUP IDENTIFIER', 'OBSERVATION DATE']\n",
    "\n",
    "sp_codes = [x.name.split('_')[1] for x in list(Path(\"/Users/tessa/Code/scratchpad/subspecies_plotter/data/\").glob(\"*.zip\"))]\n",
    "for sp_code in sp_codes:\n",
    "    print(\"\\n\\n\\nProcessing\", sp_code)\n",
    "    dataset_filepath = f\"/Users/tessa/Code/scratchpad/subspecies_plotter/data/ebd_{sp_code}_relOct-2024/ebd_{sp_code}_relOct-2024.txt\"\n",
    "\n",
    "    resolution = resolutions[0]\n",
    "\n",
    "    ssp_batch_directory = Path('/Users/tessa/Code/scratchpad/subspecies_plotter/batches/')\n",
    "    ssp_batch_directory.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read in CSV in batches\n",
    "    chunk_rows = 100000\n",
    "    tracker_filepath = f\"/Users/tessa/Code/scratchpad/subspecies_plotter/{sp_code}_tracker_rowsperchunk-{chunk_rows}.csv\"\n",
    "\n",
    "    if Path(tracker_filepath).exists():\n",
    "        tracker = pd.read_csv(tracker_filepath)\n",
    "        tracker[\"spp_to_do\"] = tracker[\"spp_to_do\"].apply(ast.literal_eval) \n",
    "        tracker[\"spp_done\"] = tracker[\"spp_done\"].apply(ast.literal_eval) \n",
    "        start_idx = tracker.index[-1]\n",
    "        spp_to_do = set(tracker.loc[start_idx].spp_to_do) - set(tracker.loc[start_idx].spp_done)\n",
    "        if spp_to_do == set():\n",
    "            skiprows = tracker.loc[start_idx].end_row\n",
    "            start_idx = start_idx + 1\n",
    "            spp_to_do = None\n",
    "        else:\n",
    "            skiprows = tracker.loc[start_idx].start_row\n",
    "\n",
    "    else:\n",
    "        tracker = pd.DataFrame(columns=[\"start_row\", \"end_row\", \"spp_to_do\", \"spp_done\"])\n",
    "        start_idx = 0\n",
    "        spp_to_do = None\n",
    "        skiprows=0\n",
    "\n",
    "    # TODO: DEAL WITH BUG (BELOW)\n",
    "    # SWITCH TO DASK TO PARALLELIZE\n",
    "    for idx, chunk in enumerate(pd.read_csv(dataset_filepath, chunksize=chunk_rows, skiprows=range(1,skiprows), usecols=use_cols, sep=\"\\t\")):\n",
    "        if chunk.shape[0] == 0:\n",
    "            print(\"No more data to process, total rows in dataset: \", (start_idx + idx)*chunk_rows)\n",
    "            break\n",
    "        if chunk.shape[0] < chunk_rows:\n",
    "            # Some kind of weird bug/issue with the last chunk \n",
    "            # which finds a single row left to process claiming to be in the next 100,000 rows after the last one\n",
    "            # This only happens after the first time I rerun this cell\n",
    "            end_row = (start_idx+idx)*chunk_rows + chunk.shape[0]\n",
    "            print(f\"Last chunk, total rows in dataset:\", end_row)\n",
    "        else:\n",
    "            end_row = (start_idx + idx)*chunk_rows+chunk_rows\n",
    "        cleaned = clean_ebd(chunk)\n",
    "        if spp_to_do == None: # Add new row\n",
    "            spp_to_do = list(set(cleaned[\"SCIENTIFIC NAME\"].unique()))\n",
    "            tracker.loc[start_idx+idx] = [(start_idx + idx)*chunk_rows, end_row, spp_to_do, []]\n",
    "\n",
    "        \n",
    "        for sp in spp_to_do:\n",
    "            cleaned_sp = cleaned[cleaned[\"SCIENTIFIC NAME\"] == sp]\n",
    "            if cleaned_sp.shape[0] == 0:\n",
    "                #print(f\"No data for {sp}\")\n",
    "                continue\n",
    "\n",
    "            # Get list of subspecies\n",
    "            subspp = []\n",
    "            for k, val in spp_dict[sp]['infraspecies'].items():\n",
    "                subspp.extend(val.keys())\n",
    "            \n",
    "            # Get data on presence of each subspp for each resolution\n",
    "            for resolution in resolutions:\n",
    "                species_df = get_species_df(sp, cleaned_sp, subspp, resolution)\n",
    "                filename = ssp_batch_directory.joinpath(f'{sp}_row{(start_idx+idx)*chunk_rows}-{end_row}_resolution{resolution}.csv')\n",
    "                species_df.to_csv(filename)\n",
    "\n",
    "            tracker.loc[start_idx+idx].spp_done += [sp]\n",
    "            tracker.to_csv(tracker_filepath, index=False)\n",
    "\n",
    "        spp_to_do = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum up the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cell_df_directory = Path('/Users/tessa/Code/scratchpad/subspecies_plotter/sp_cell_dfs/')\n",
    "sp_cell_df_directory.mkdir(exist_ok=True)\n",
    "\n",
    "def parse_batch_files(ssp_batch_directory):\n",
    "    batch_files = list(ssp_batch_directory.glob(\"*.csv\"))\n",
    "    file_info = [n.name.split(\"_\") for n in batch_files]\n",
    "    files = pd.DataFrame(file_info, columns=['SCIENTIFIC NAME', 'ROW RANGE', 'RESOLUTION'])\n",
    "    files['FILENAME'] = batch_files\n",
    "    for (species, resolution), species_df in files.groupby([\"SCIENTIFIC NAME\", 'RESOLUTION']):\n",
    "        species = species.replace(\" \", \"-\")\n",
    "        resolution = resolution[:-4]\n",
    "        all_dataframes = [pd.read_csv(f, index_col=0) for f in species_df.FILENAME] \n",
    "        sp_cell_df = reduce(lambda a, b: a.add(b, fill_value=0), all_dataframes)\n",
    "        filename = sp_cell_df_directory.joinpath(f'{species}_{resolution}.csv')\n",
    "        sp_cell_df.to_csv(filename)\n",
    "\n",
    "\n",
    "\n",
    "parse_batch_files(ssp_batch_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def name_to_base_hue(name):\n",
    "    \"\"\"Generate a base hue from a name.\"\"\"\n",
    "    base_hue = hash(name) % 360\n",
    "    return base_hue\n",
    "\n",
    "def average_hues(hues):\n",
    "    \"\"\"Average a list of hues on the circular scale.\"\"\"\n",
    "    x = np.mean([np.cos(np.radians(h)) for h in hues])\n",
    "    y = np.mean([np.sin(np.radians(h)) for h in hues])\n",
    "    avg_hue = np.degrees(np.arctan2(y, x)) % 360\n",
    "    return avg_hue\n",
    "\n",
    "# def assign_colors(subspecies, overlap_matrix):\n",
    "#     \"\"\"Assign colors to subspecies based on overlap relationships.\"\"\"\n",
    "#     # Step 1: Generate base hues\n",
    "#     base_hues = {subsp: name_to_base_hue(subsp) for subsp in subspecies}\n",
    "    \n",
    "#     # Step 2: Adjust hues based on overlap using graph coloring\n",
    "#     G = nx.Graph()\n",
    "#     for i, sp1 in enumerate(subspecies):\n",
    "#         for j, sp2 in enumerate(subspecies):\n",
    "#             if overlap_matrix[i][j] > 0.1:  # Threshold for \"overlap\"\n",
    "#                 G.add_edge(sp1, sp2)\n",
    "    \n",
    "#     coloring = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n",
    "#     color_mapping = {}\n",
    "    \n",
    "#     for subsp, color_idx in coloring.items():\n",
    "#         hue = (base_hues[subsp] + color_idx * 60) % 360  # Spread hues by 60Â° to maximize contrast\n",
    "#         saturation, lightness = 0.8, 0.5  # Vivid, medium colors\n",
    "#         r, g, b = colorsys.hls_to_rgb(hue / 360, lightness, saturation)\n",
    "#         color_mapping[subsp] = rgb_to_hex((int(r * 255), int(g * 255), int(b * 255)))\n",
    "    \n",
    "#     return color_mapping\n",
    "from colorsys import hsv_to_rgb\n",
    "import numpy as np\n",
    "\n",
    "def adjust_hue_similarity(colors, min_hue_diff=0.2):\n",
    "    \"\"\"\n",
    "    Adjust colors to ensure hues are sufficiently different.\n",
    "    \n",
    "    Parameters:\n",
    "    - colors: A list of (hue, saturation, value) tuples.\n",
    "    - min_hue_diff: Minimum allowed difference between any two hues.\n",
    "    \n",
    "    Returns:\n",
    "    - adjusted_colors: A list of adjusted (hue, saturation, value) tuples.\n",
    "    \"\"\"\n",
    "    adjusted_colors = []\n",
    "    for i, color in enumerate(colors):\n",
    "        hue, sat, val = color\n",
    "        too_close = any(abs(hue - other_hue) < min_hue_diff for other_hue, _, _ in adjusted_colors)\n",
    "        \n",
    "        if too_close:\n",
    "            # Adjust saturation and brightness to differentiate\n",
    "            sat = max(0.5, sat * 0.5)  # Slightly desaturate\n",
    "            val = min(1.0, val * 1.2)  # Brighten slightly\n",
    "        \n",
    "        adjusted_colors.append((hue, sat, val))\n",
    "    return adjusted_colors\n",
    "\n",
    "def generate_distinct_colors_with_hue_adjustment(adjacency_matrix, subspecies_names, min_hue_diff=0.1):\n",
    "    \"\"\"\n",
    "    Generate distinct colors for each subspecies with adjustments for hue similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_matrix: The adjacency matrix for subspecies.\n",
    "    - subspecies_names: A list of subspecies names corresponding to the matrix indices.\n",
    "    - min_hue_diff: Minimum hue difference between any two subspecies.\n",
    "    \n",
    "    Returns:\n",
    "    - subspecies_colors: A dictionary mapping subspecies names to RGB colors.\n",
    "    \"\"\"\n",
    "    n = len(adjacency_matrix)\n",
    "    initial_hues = np.linspace(0, 1, n, endpoint=False)  # Evenly spaced hues\n",
    "    \n",
    "    # Sort subspecies by adjacency density to prioritize distinction\n",
    "    adjacency_density = adjacency_matrix.sum(axis=1)\n",
    "    sorted_indices = np.argsort(-adjacency_density)\n",
    "    sorted_hues = initial_hues[sorted_indices]\n",
    "    \n",
    "    # Assign initial HSV values\n",
    "    hsv_colors = [(hue, 0.7, 0.9) for hue in sorted_hues]  # Start with vivid colors\n",
    "    \n",
    "    # Adjust for hue similarity\n",
    "    adjusted_hsv_colors = adjust_hue_similarity(hsv_colors, min_hue_diff=min_hue_diff)\n",
    "    \n",
    "    # Convert HSV to RGB and map to subspecies names\n",
    "    rgb_colors = {\n",
    "        subspecies_names[idx]: rgb_to_hex(tuple(int(c * 255) for c in hsv_to_rgb(*adjusted_hsv_colors[i])))\n",
    "        for i, idx in enumerate(sorted_indices)\n",
    "    }\n",
    "    \n",
    "    return rgb_colors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_distribution_adjacency_matrix(data, subspecies_cols, cell_col='cell_id'):\n",
    "    \"\"\"\n",
    "    Create an adjacency matrix based on subspecies distribution similarities.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame with cells as rows and subspecies counts as columns.\n",
    "    - subspecies_cols: List of column names corresponding to subspecies counts.\n",
    "    - cell_col: Column name for cell identifiers (optional, for reference).\n",
    "\n",
    "    Returns:\n",
    "    - adjacency_matrix: A NumPy array where element [i, j] is the similarity between subspecies distributions.\n",
    "    - subspecies_list: The order of subspecies corresponding to matrix rows/columns.\n",
    "    \"\"\"\n",
    "    # Subset the subspecies columns\n",
    "    subspecies_data = data[subspecies_cols]\n",
    "\n",
    "    # Normalize each cell's counts to proportions\n",
    "    subspecies_distribution = subspecies_data.div(subspecies_data.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # Compute cosine similarity between each pair of subspecies\n",
    "    adjacency_matrix = cosine_similarity(subspecies_distribution.T)\n",
    "\n",
    "    # Return the matrix and list of subspecies\n",
    "    return adjacency_matrix, subspecies_cols\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert hex color (#RRGGBB) to an (R, G, B) tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Convert an (R, G, B) tuple to a hex color (#RRGGBB).\"\"\"\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(*rgb)\n",
    "\n",
    "def combine_rgb_colors(rgb_colors, fracs):\n",
    "    \"\"\"Combine a list of RGB colors proportionally.\"\"\"\n",
    "    if sum(fracs) == 0:\n",
    "        return \"#999999\"\n",
    "    else:\n",
    "        combined_rgb = tuple(\n",
    "            int(sum(frac * color[channel] for color, frac in zip(rgb_colors, fracs)))\n",
    "            for channel in range(3)\n",
    "        )\n",
    "    return rgb_to_hex(combined_rgb)\n",
    "\n",
    "def style_function(feature, subspp_colors):\n",
    "    \"\"\"Style a cell based on the proportion of subspecies.\"\"\"\n",
    "    properties = feature['properties']\n",
    "    subspecies_values = {subsp: properties.get(subsp, 0) for subsp in subspp_colors.keys()}\n",
    "    \n",
    "    # Normalize the values to sum up to 1 for proportional allocation\n",
    "    total = sum(subspecies_values.values())\n",
    "    if total > 0:\n",
    "        fracs = [value / total for value in subspecies_values.values()]\n",
    "    else:\n",
    "        fracs = [0 for _ in subspecies_values]\n",
    "    \n",
    "    # Get RGB colors for each subspecies\n",
    "    hex_colors = [subspp_colors[subsp] for subsp in subspecies_values]\n",
    "    rgb_colors = [hex_to_rgb(color) for color in hex_colors]\n",
    "    \n",
    "    # Combine colors based on the proportional fractions\n",
    "    cell_color = combine_rgb_colors(rgb_colors, fracs)\n",
    "    \n",
    "    return {\n",
    "        'fillColor': cell_color,  # Cell color\n",
    "        'color': cell_color,  # Border color\n",
    "        'weight': 1,  # Border weight\n",
    "        'fillOpacity': 0.6,  # Cell fill transparency\n",
    "    }\n",
    "\n",
    "\n",
    "def choropleth_map(sp_cell_df, common_name, adjacency_matrix):\n",
    "    \"\"\"Creates a choropleth map given species data.\"\"\"\n",
    "    \n",
    "    f = folium.Figure()\n",
    "    map = folium.Map(location=[47, -122], zoom_start=5, tiles=\"cartodbpositron\")\n",
    "    f.add_child(map)\n",
    "\n",
    "    sp = sp_cell_df.columns[0]\n",
    "    subspp = sp_cell_df.columns[1:]\n",
    "    \n",
    "\n",
    "    # adjacency_matrix, subspp = create_distribution_adjacency_matrix(sp_cell_df, subspp)\n",
    "\n",
    "\n",
    "\n",
    "    subspp_colors = generate_distinct_colors_with_hue_adjustment(adjacency_matrix, subspp)\n",
    "\n",
    "    # subspp_colors = assign_colors(subspp, adjacency_matrix)\n",
    "\n",
    "    # # Generate random colors for each subspecies\n",
    "    # subspp_colors = {subsp: generate_color_from_name(subsp) for subsp in subspp}\n",
    "    \n",
    "    list_features = []\n",
    "    for _, row in sp_cell_df.iterrows():\n",
    "        #percentages = (row[subspp] / row[sp]) # For the previous implementation that colored the map by % of total sightings instead of % of ssp sightings\n",
    "        percentages = (row[subspp] / sum(row[subspp]))*100\n",
    "        percentages_dict = percentages.to_dict()\n",
    "        \n",
    "        # Precompute tooltip text showing only non-zero percentages\n",
    "        percentages_dict_ordered = pd.DataFrame(percentages_dict, index=['pct']).T.query('pct > 0')['pct'].sort_values(ascending=False).to_dict()\n",
    "\n",
    "        tooltip_text = []\n",
    "        for subsp, percent in percentages_dict_ordered.items():\n",
    "            tooltip_text.append(f\"{subsp}: {percent:.0f}%\")\n",
    "        \n",
    "        # Add tooltip as a string to the properties\n",
    "        percentages_dict[\"tooltip\"] = \"<br>\".join(tooltip_text) if tooltip_text else \"No data\"\n",
    "\n",
    "        geometry_for_row = h3.cells_to_geo(cells=[row.name])\n",
    "        feature = Feature(\n",
    "            geometry=geometry_for_row,\n",
    "            id=row.name,\n",
    "            properties=percentages_dict)\n",
    "        list_features.append(feature)\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "    \n",
    "    # Add GeoJSON layer to the map\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: style_function(feature, subspp_colors),\n",
    "        name=f'{sp} Subspecies Map'\n",
    "    ).add_to(map)\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: {\n",
    "            'weight': 0,  # No border weight\n",
    "            'color': 'transparent',  # No border color\n",
    "            'fillOpacity': 0.6  # Fill transparency\n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            #fields=list(subspp),\n",
    "            #aliases=[subsp[len(sp)+1:] for subsp in subspp], # Removes ssp name\n",
    "            fields=[\"tooltip\"],\n",
    "            aliases=[\"Reported\\nSubspecies\"],\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            labels=True,\n",
    "            labels_format=\"{:.2f}%\",\n",
    "            #highlight_function=lambda x: x.update({'text': [f'{k}: {v:.2f}%' for k, v in x['properties'].items() if v > 0]})\n",
    "        )\n",
    "    ).add_to(map)\n",
    "\n",
    "\n",
    "    # Add legend\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"position: fixed; top: 10px; right: 10px; width: 150px; height: auto; z-index: 9999; background-color: white; box-shadow: 0 0 5px rgba(0, 0, 0, 0.2); border: 1px solid lightgray; border-radius: 5px; padding: 10px; font-size: 10px;\">\n",
    "        <strong>{common_name} infraspecies</strong><br>\n",
    "    \"\"\"\n",
    "    for subsp, color in subspp_colors.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <span style=\"display: inline-block; width: 20px; height: 10px; margin-right: 10px; background-color: {color};\"></span>\n",
    "            {subsp}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    legend_html += \"</div>\"\n",
    "    legend_element = folium.Element(legend_html)\n",
    "    map.get_root().html.add_child(legend_element)\n",
    "\n",
    "    # Calculate bounds and adjust the map's view\n",
    "    bounds = get_bounds(geojson_result)\n",
    "    map.fit_bounds(bounds)\n",
    "\n",
    "    return map\n",
    "\n",
    "# Example usage:\n",
    "#for sp_code in sp_codes:\n",
    "remake_maps = False\n",
    "for sp_code in sp_codes:\n",
    "    common_name = taxonomy[taxonomy['SPECIES_CODE'] == sp_code].PRIMARY_COM_NAME.values[0]\n",
    "    for resolution in [2,3,4,5]:\n",
    "        species = taxonomy[taxonomy['PRIMARY_COM_NAME'] == common_name].SCI_NAME.values[0]\n",
    "        dataname = f\"sp_cell_dfs/{species.replace(' ', '-')}_resolution{resolution}.csv\"\n",
    "        if not Path(dataname).exists():\n",
    "            continue\n",
    "        map_filename = f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\"\n",
    "        if Path(map_filename).exists() and not remake_maps:\n",
    "            continue\n",
    "        sp_cell_df = pd.read_csv(dataname, index_col=0)\n",
    "        sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "        subspecies = sp_cell_df.columns[1:]\n",
    "        if resolution == 2:\n",
    "            adjacency_matrix, subspecies = create_distribution_adjacency_matrix(sp_cell_df, subspecies)\n",
    "\n",
    "        m = choropleth_map(sp_cell_df, common_name, adjacency_matrix)\n",
    "        m.save(f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CSV of map URLs for the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maps/Butorides-striata_4.html\n",
      "maps/Butorides-striata_5.html\n",
      "maps/Butorides-striata_2.html\n",
      "maps/Butorides-striata_3.html\n",
      "maps/Branta-bernicla_3.html\n",
      "maps/Setophaga-coronata_3.html\n",
      "maps/Buteo-jamaicensis_4.html\n",
      "maps/Zonotrichia-leucophrys_3.html\n",
      "maps/Sturnella-magna_5.html\n",
      "maps/Loxia-curvirostra_3.html\n",
      "maps/Garrulus-glandarius_4.html\n",
      "maps/Garrulus-glandarius_5.html\n",
      "maps/Loxia-curvirostra_2.html\n",
      "maps/Sturnella-magna_4.html\n",
      "maps/Zonotrichia-leucophrys_2.html\n",
      "maps/Buteo-jamaicensis_5.html\n",
      "maps/Setophaga-coronata_2.html\n",
      "maps/Branta-bernicla_2.html\n",
      "maps/Sturnella-magna_3.html\n",
      "maps/Loxia-curvirostra_5.html\n",
      "maps/Garrulus-glandarius_2.html\n",
      "maps/Branta-bernicla_5.html\n",
      "maps/Setophaga-coronata_5.html\n",
      "maps/Buteo-jamaicensis_2.html\n",
      "maps/Branta-hutchinsii_2.html\n",
      "maps/Zonotrichia-leucophrys_5.html\n",
      "maps/Zonotrichia-leucophrys_4.html\n",
      "maps/Buteo-jamaicensis_3.html\n",
      "maps/Setophaga-coronata_4.html\n",
      "maps/Branta-bernicla_4.html\n",
      "maps/Garrulus-glandarius_3.html\n",
      "maps/Loxia-curvirostra_4.html\n",
      "maps/Sturnella-magna_2.html\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"common_name\", \"scientific_name\", \"resolution\", \"map_url\"])\n",
    "maps_dir = Path(\"/Users/tessa/Code/scratchpad/subspecies_plotter/docs/maps\")\n",
    "for idx, file in enumerate(maps_dir.glob(\"*.html\")):\n",
    "    resolution = file.stem.split(\"_\")[-1]\n",
    "    species = file.stem.replace(f\"_{resolution}\", \"\")\n",
    "    common_name = taxonomy[taxonomy['SCI_NAME'] == species.replace('-', ' ')].PRIMARY_COM_NAME.values[0]\n",
    "    map_url = Path(Path(file).parent.stem).joinpath(Path(file).name)\n",
    "    print(map_url)\n",
    "    df.loc[idx] = [common_name, species, resolution, map_url]\n",
    "df.to_csv(\"docs/data/map_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old color creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# def generate_random_color():\n",
    "#     \"\"\"Generates a random color in hex format.\"\"\"\n",
    "#     return \"#{:02x}{:02x}{:02x}\".format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_color_from_name(name, luminance=0.5, saturation=0.8):\n",
    "    \"\"\"\n",
    "    Generate a vivid, medium color based on a subspecies name using its hash value.\n",
    "    \n",
    "    Args:\n",
    "    - name: The subspecies name (string) used to generate a unique color.\n",
    "    - luminance: The luminance of the color (0 = dark, 1 = light, default is medium).\n",
    "    - saturation: The saturation of the color (0 = grey, 1 = full saturation, default is high saturation).\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the color in HEX format.\n",
    "    \"\"\"\n",
    "    # Create a hash from the subspecies name\n",
    "    hash_value = int(hashlib.sha256(name.encode('utf-8')).hexdigest(), 16)\n",
    "    \n",
    "    # Normalize the hash to be between 0 and 1 for hue (360 degrees for hue)\n",
    "    hue = ((hash_value + 100) % 360) / 360.0\n",
    "\n",
    "    # Normalize the hash for luminance and saturation\n",
    "    #luminance = (hash_value % 100) / 200.0 + 0.45\n",
    "    #saturation = (hash_value % 100) / 200.0 + 0.5\n",
    "    \n",
    "    # Generate the color in HLS (Hue, Lightness, Saturation) space\n",
    "    r, g, b = colorsys.hls_to_rgb(hue, luminance, saturation)\n",
    "    \n",
    "    # Convert the RGB color to hexadecimal\n",
    "    return rgb_to_hex((int(r*255), int(g*255), int(b*255)))\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert hex color (#RRGGBB) to an (R, G, B) tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    \"\"\"Convert an (R, G, B) tuple to a hex color (#RRGGBB).\"\"\"\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(*rgb)\n",
    "\n",
    "def combine_rgb_colors(rgb_colors, fracs):\n",
    "    \"\"\"Combine a list of RGB colors proportionally.\"\"\"\n",
    "    if sum(fracs) == 0:\n",
    "        return \"#999999\"\n",
    "    else:\n",
    "        combined_rgb = tuple(\n",
    "            int(sum(frac * color[channel] for color, frac in zip(rgb_colors, fracs)))\n",
    "            for channel in range(3)\n",
    "        )\n",
    "    return rgb_to_hex(combined_rgb)\n",
    "\n",
    "def style_function(feature, subspp_colors):\n",
    "    \"\"\"Style a cell based on the proportion of subspecies.\"\"\"\n",
    "    properties = feature['properties']\n",
    "    subspecies_values = {subsp: properties.get(subsp, 0) for subsp in subspp_colors.keys()}\n",
    "    \n",
    "    # Normalize the values to sum up to 1 for proportional allocation\n",
    "    total = sum(subspecies_values.values())\n",
    "    if total > 0:\n",
    "        fracs = [value / total for value in subspecies_values.values()]\n",
    "    else:\n",
    "        fracs = [0 for _ in subspecies_values]\n",
    "    \n",
    "    # Get RGB colors for each subspecies\n",
    "    hex_colors = [subspp_colors[subsp] for subsp in subspecies_values]\n",
    "    rgb_colors = [hex_to_rgb(color) for color in hex_colors]\n",
    "    \n",
    "    # Combine colors based on the proportional fractions\n",
    "    cell_color = combine_rgb_colors(rgb_colors, fracs)\n",
    "    \n",
    "    return {\n",
    "        'fillColor': cell_color,  # Cell color\n",
    "        'color': cell_color,  # Border color\n",
    "        'weight': 1,  # Border weight\n",
    "        'fillOpacity': 0.6,  # Cell fill transparency\n",
    "    }\n",
    "\n",
    "\n",
    "def choropleth_map(sp_cell_df, common_name):\n",
    "    \"\"\"Creates a choropleth map given species data.\"\"\"\n",
    "    \n",
    "    f = folium.Figure()\n",
    "    map = folium.Map(location=[47, -122], zoom_start=5, tiles=\"cartodbpositron\")\n",
    "    f.add_child(map)\n",
    "\n",
    "    sp = sp_cell_df.columns[0]\n",
    "    subspp = sp_cell_df.columns[1:]\n",
    "    \n",
    "    # Generate random colors for each subspecies\n",
    "    subspp_colors = {subsp: generate_color_from_name(subsp) for subsp in subspp}\n",
    "    \n",
    "    list_features = []\n",
    "    for _, row in sp_cell_df.iterrows():\n",
    "        #percentages = (row[subspp] / row[sp]) # For the previous implementation that colored the map by % of total sightings instead of % of ssp sightings\n",
    "        percentages = (row[subspp] / sum(row[subspp]))*100\n",
    "        percentages_dict = percentages.to_dict()\n",
    "        \n",
    "        # Precompute tooltip text showing only non-zero percentages\n",
    "        percentages_dict_ordered = pd.DataFrame(percentages_dict, index=['pct']).T.query('pct > 0')['pct'].sort_values(ascending=False).to_dict()\n",
    "\n",
    "        tooltip_text = []\n",
    "        for subsp, percent in percentages_dict_ordered.items():\n",
    "            tooltip_text.append(f\"{subsp}: {percent:.0f}%\")\n",
    "        \n",
    "        # Add tooltip as a string to the properties\n",
    "        percentages_dict[\"tooltip\"] = \"<br>\".join(tooltip_text) if tooltip_text else \"No data\"\n",
    "\n",
    "        geometry_for_row = h3.cells_to_geo(cells=[row.name])\n",
    "        feature = Feature(\n",
    "            geometry=geometry_for_row,\n",
    "            id=row.name,\n",
    "            properties=percentages_dict)\n",
    "        list_features.append(feature)\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "    \n",
    "    # Add GeoJSON layer to the map\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: style_function(feature, subspp_colors),\n",
    "        name=f'{sp} Subspecies Map'\n",
    "    ).add_to(map)\n",
    "    \n",
    "    # Add tooltips\n",
    "    folium.GeoJson(\n",
    "        geojson_result,\n",
    "        style_function=lambda feature: {\n",
    "            'weight': 0,  # No border weight\n",
    "            'color': 'transparent',  # No border color\n",
    "            'fillOpacity': 0.6  # Fill transparency\n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            #fields=list(subspp),\n",
    "            #aliases=[subsp[len(sp)+1:] for subsp in subspp], # Removes ssp name\n",
    "            fields=[\"tooltip\"],\n",
    "            aliases=[\"Reported\\nSubspecies\"],\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            labels=True,\n",
    "            labels_format=\"{:.2f}%\",\n",
    "            #highlight_function=lambda x: x.update({'text': [f'{k}: {v:.2f}%' for k, v in x['properties'].items() if v > 0]})\n",
    "        )\n",
    "    ).add_to(map)\n",
    "\n",
    "\n",
    "    # Add legend\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"position: fixed; top: 10px; right: 10px; width: 150px; height: auto; z-index: 9999; background-color: white; box-shadow: 0 0 5px rgba(0, 0, 0, 0.2); border: 1px solid lightgray; border-radius: 5px; padding: 10px; font-size: 10px;\">\n",
    "        <strong>{common_name} subspecies</strong><br>\n",
    "    \"\"\"\n",
    "    for subsp, color in subspp_colors.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <div style=\"margin-top: 10px;\">\n",
    "            <span style=\"display: inline-block; width: 20px; height: 10px; margin-right: 10px; background-color: {color};\"></span>\n",
    "            {subsp}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    legend_html += \"</div>\"\n",
    "    legend_element = folium.Element(legend_html)\n",
    "    map.get_root().html.add_child(legend_element)\n",
    "\n",
    "    # Calculate bounds and adjust the map's view\n",
    "    bounds = get_bounds(geojson_result)\n",
    "    map.fit_bounds(bounds)\n",
    "\n",
    "    return map\n",
    "\n",
    "# Example usage:\n",
    "for sp_code in sp_codes:\n",
    "    common_name = taxonomy[taxonomy['SPECIES_CODE'] == sp_code].PRIMARY_COM_NAME.values[0]\n",
    "    for resolution in [2,3,4,5]:\n",
    "        species = taxonomy[taxonomy['PRIMARY_COM_NAME'] == common_name].SCI_NAME.values[0]\n",
    "        dataname = f\"sp_cell_dfs/{species.replace(' ', '-')}_resolution{resolution}.csv\"\n",
    "        if not Path(dataname).exists():\n",
    "            continue\n",
    "        sp_cell_df = pd.read_csv(dataname, index_col=0)\n",
    "        sp_cell_df.columns = sp_cell_df.columns.str.replace(species + ' ', \"\")\n",
    "        m = choropleth_map(sp_cell_df, common_name)\n",
    "        m.save(f\"docs/maps/{species.replace(' ', '-')}_{resolution}.html\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
